<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Binary classification · GaussianProcesses.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">GaussianProcesses.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><span class="tocitem">Basic usage</span><ul><li><a class="tocitem" href="../Regression/">Simple GP Regression</a></li><li><a class="tocitem" href="../plotting_gps/">Plotting with GaussianProcesses.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Binary classification</a></li><li><a class="tocitem" href="../sparse_example/">Sparse GPs</a></li><li><a class="tocitem" href="../mauna_loa/">Time series example with Mauna Loa data</a></li><li><a class="tocitem" href="../poisson_regression/">Poisson Regression example</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../gp/">Gaussian Processes</a></li><li><a class="tocitem" href="../kernels/">Kernels</a></li><li><a class="tocitem" href="../mean/">Means</a></li><li><a class="tocitem" href="../lik/">Likelihoods</a></li><li><a class="tocitem" href="../optimization/">Optimization</a></li><li><a class="tocitem" href="../sparse/">Sparse GPs</a></li><li><a class="tocitem" href="../crossvalidation/">Cross Validation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Binary classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Binary classification</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/STOR-i/GaussianProcesses.jl/blob/master/docs/src/classification_example.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Binary-classification-1"><a class="docs-heading-anchor" href="#Binary-classification-1">Binary classification</a><a class="docs-heading-anchor-permalink" href="#Binary-classification-1" title="Permalink"></a></h1><p>The package is designed to handle models of the following general form</p><div>\[\begin{aligned}
\mathbf{y}  \ |\ \mathbf{f}, \theta &amp;\sim  \prod_{i=1}^n p(y_i \ | \ f_i,\theta), \\
\mathbf{f}  \ | \ \theta &amp;\sim \mathcal{GP}\left(m_{\theta}(\mathbf{x}), k_{\theta}(\mathbf{x}, \mathbf{x}&#39;)\right),\\
\theta &amp; \sim p(\theta),
\end{aligned}\]</div><p>where <span>$\mathbf{y}=(y_1,y_2,\ldots,y_n) \in \mathcal{Y}$</span> and <span>$\mathbf{x} \in \mathcal{X}$</span> are the observations and covariates, respectively, and <span>$f_i:=f(\mathbf{x}_i)$</span> is the latent function which we model with a Gaussian process prior. We assume that the responses <span>$\mathbf{y}$</span> are independent and identically distributed and as a result the likelihood <span>$p(\mathbf{y} \ | \ \mathbf{f}, \theta)$</span>, can be factorized over the observations.</p><p>In the case where the observations are Gaussian distributed, the marginal likelihood and predictive distribution can be derived analytically. See the <a href="http://stor-i.github.io/GaussianProcesses.jl/latest/Regression.html">Regression documentation</a> for an illustration.</p><p>In this example we show how the GP <strong>Monte Carlo</strong> function can be used for <strong>supervised learning classification</strong>. We use the Crab dataset from the R package MASS. In this dataset we are interested in predicting whether a crab is of colour form blue or orange. Our aim is to perform a Bayesian analysis and calculate the posterior distribution of the latent GP function <span>$\mathbf{f}$</span> and model parameters <span>$\theta$</span> from the training data <span>$\{\mathbf{X}, \mathbf{y}\}$</span>.</p><pre><code class="language-julia">using GaussianProcesses, RDatasets
import Distributions:Normal
using Random

Random.seed!(113355)

crabs = dataset(&quot;MASS&quot;,&quot;crabs&quot;);              # load the data
crabs = crabs[shuffle(1:size(crabs)[1]), :];  # shuffle the data

train = crabs[1:div(end,2),:];

y = Array{Bool}(undef,size(train)[1]);       # response
y[train[:,:Sp].==&quot;B&quot;].=0;                      # convert characters to booleans
y[train[:,:Sp].==&quot;O&quot;].=1;

X = convert(Matrix,train[:,4:end]);          # predictors</code></pre><p>We assume a zero mean GP with a Matern 3/2 kernel. We use the automatic relevance determination (ARD) kernel to allow each dimension of the predictor variables to have a different length scale. As this is binary classifcation, we use the Bernoulli likelihood,</p><div>\[y_i \sim \mbox{Bernoulli}(\Phi(f_i))\]</div><p>where <span>$\Phi: \mathbb{R} \rightarrow [0,1]$</span> is the cumulative distribution function of a standard Gaussian and acts as a squash function that maps the GP function to the interval [0,1], giving the probability that <span>$y_i=1$</span>.</p><p><strong>Note</strong> that <code>BernLik</code> requires the observations to be of type <code>Bool</code> and unlike some likelihood functions (e.g. student-t) does not contain any parameters to be set at initialisation.</p><pre><code class="language-julia">#Select mean, kernel and likelihood function
mZero = MeanZero();                # Zero mean function
kern = Matern(3/2,zeros(5),0.0);   # Matern 3/2 ARD kernel (note that hyperparameters are on the log scale)
lik = BernLik();                   # Bernoulli likelihood for binary data {0,1}</code></pre><p>We fit the GP using the general <code>GP</code> function. This function is a shorthand for the <code>GPA</code> function which is used to generate <strong>approximations</strong> of the latent function when the <strong>likelihood is non-Gaussian</strong>.</p><pre><code class="language-julia">gp = GP(X&#39;,y,mZero,kern,lik)      # Fit the Gaussian process model</code></pre><pre><code class="language-none">GP Approximate object:
  Dim = 5
  Number of observations = 100
  Mean function:
    Type: MeanZero, Params: Float64[]
  Kernel:
    Type: Mat32Ard{Float64}, Params: [-0.0, -0.0, -0.0, -0.0, -0.0, 0.0]
  Likelihood:
    Type: BernLik, Params: Any[]
  Input observations =
[16.2 11.2 … 11.6 18.5; 13.3 10.0 … 9.1 14.6; … ; 41.7 26.9 … 28.4 42.0; 15.4 9.4 … 10.4 16.6]
  Output observations = Bool[false, false, false, false, true, true, false, true, true, true  …  false, true, false, false, false, true, false, false, false, true]
  Log-posterior = -161.209</code></pre><p>We assign <code>Normal</code> priors from the <code>Distributions</code> package to each of the Matern kernel parameters. If the mean and likelihood function also contained parameters, then we could set these priors in the same using <code>gp.m</code> and <code>gp.lik</code> in place of <code>gp.k</code>, respectively.</p><pre><code class="language-julia">set_priors!(gp.kernel,[Normal(0.0,2.0) for i in 1:6])</code></pre><pre><code class="language-none">6-element Array{Normal{Float64},1}:
 Normal{Float64}(μ=0.0, σ=2.0)
 Normal{Float64}(μ=0.0, σ=2.0)
 Normal{Float64}(μ=0.0, σ=2.0)
 Normal{Float64}(μ=0.0, σ=2.0)
 Normal{Float64}(μ=0.0, σ=2.0)
 Normal{Float64}(μ=0.0, σ=2.0)</code></pre><p>Samples of the latent function <span>$f,\theta \ | \ X,y$</span> are drawn using MCMC sampling. By default, the <code>mcmc</code> function uses the Hamiltonian Monte Carlo algorithm. Unless defined, the default settings for the MCMC sampler are: 1,000 iterations with no burn-in phase or thinning of the Markov chain.</p><pre><code class="language-julia">samples = mcmc(gp; nIter=10000, burn=1000, thin=10);</code></pre><pre><code class="language-none">Number of iterations = 10000, Thinning = 10, Burn-in = 1000
Step size = 0.100000, Average number of leapfrog steps = 10.015100
Number of function calls: 100152
Acceptance rate: 0.087600</code></pre><p>We test the predictive accuracy of the fitted model against a hold-out dataset</p><pre><code class="language-julia">test = crabs[div(end,2)+1:end,:];          # select test data

yTest = Array{Bool}(undef,size(test)[1]);   # test response data
yTest[test[:,:Sp].==&quot;B&quot;].=0;                  # convert characters to booleans
yTest[test[:,:Sp].==&quot;O&quot;].=1;

xTest = convert(Matrix,test[:,4:end]);</code></pre><p>Using the posterior samples <span>$\{f^{(i)},\theta^{(i)}\}_{i=1}^N$</span> from <span>$p(f,\theta \ | \ X,y)$</span> we can make predictions <span>$\hat{y}$</span> using the <code>predict_y</code> function to sample predictions conditional on the MCMC samples.</p><pre><code class="language-julia">ymean = Array{Float64}(undef,size(samples,2),size(xTest,1));

for i in 1:size(samples,2)
    set_params!(gp,samples[:,i])
    update_target!(gp)
    ymean[i,:] = predict_y(gp,xTest&#39;)[1]
end</code></pre><p>For each of the posterior samples we plot the predicted observation <span>$\hat{y}$</span> (given as lines) and overlay the true observations from the held-out data (circles).</p><pre><code class="language-julia">using Plots
gr()

plot(ymean&#39;,leg=false,html_output_format=:png)
scatter!(yTest)</code></pre><p><img src="../Classification_files/Classification_17_0.png" alt="png"/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../plotting_gps/">« Plotting with GaussianProcesses.jl</a><a class="docs-footer-nextpage" href="../sparse_example/">Sparse GPs »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 11 December 2019 16:01">Wednesday 11 December 2019</span>. Using Julia version 1.1.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
