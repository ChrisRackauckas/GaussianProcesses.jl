var documenterSearchIndex = {"docs":
[{"location":"mauna_loa/#Time-series-example-with-Mauna-Loa-data-1","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"","category":"section"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"Gaussian processes can be used to model nonlinear time series. We consider the problem of predicting future concentrations of CO_2 in the atmosphere. The data are taken from the Mauna Loa observatory in Hawaii which records the monthly average atmospheric concentration of CO_2 (in parts per million) between 1958 to 2015. The purposes of testing the predictive accuracy of the Gaussian process model, we fit the GP to the historical data from 1958 to 2004 and optimise the parameters using maximum likelihood estimation. ","category":"page"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"We employ a seemingly complex kernel function to model these data which follows the kernel structure given in Rasmussen and Williams (2006). The kernel comprises of simpler kernels with each kernel term accounting for a different aspect in the variation of the data. For example, the Periodic kernel captures the seasonal effect of CO_2 absorption from plants. A detailed description of each kernel contribution is given in Chapter 5 of Rasmussen and Williams (2006).","category":"page"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"using GaussianProcesses\nusing DelimitedFiles\n\ndata = readdlm(\"data/CO2_data.csv\",',')\n\nyear = data[:,1]; co2 = data[:,2];\n#Split the data into training and testing data\nxtrain = year[year.<2004]; ytrain = co2[year.<2004];\nxtest = year[year.>=2004]; ytest = co2[year.>=2004];\n\n#Kernel is represented as a sum of kernels\nkernel = SE(4.0,4.0) + Periodic(0.0,1.0,0.0)*SE(4.0,0.0) + RQ(0.0,0.0,-1.0) + SE(-2.0,-2.0);\n\ngp = GP(xtrain,ytrain,MeanZero(),kernel,-2.0)   #Fit the GP\n\noptimize!(gp) #Estimate the parameters through maximum likelihood estimation\n\nμ, Σ = predict_y(gp,xtest);","category":"page"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"Over the ten year predidction horizon the GP is able to accurately capture both the trend and seasonal variations of the CO_2 concentrations. Arguably, the GP prediction gradually begins to underestimate the CO_2 concentration. The accuracy of the fit could be further improved by extending the kernel function to include additionally terms. Recent work on automatic structure discovery (Duvenaud et al., 2013) could be used to optimise the modelling process.","category":"page"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"using Plots\npyplot()\n\nplot(xtest,μ,ribbon=Σ, title=\"Time series prediction\",label=\"95% predictive confidence region\")\nscatter!(xtest,ytest,label=\"Observations\")","category":"page"},{"location":"mauna_loa/#","page":"Time series example with Mauna Loa data","title":"Time series example with Mauna Loa data","text":"(Image: png)","category":"page"},{"location":"optimization/#Optimization-1","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"optimization/#","page":"Optimization","title":"Optimization","text":"Modules = [GaussianProcesses]\nPages = [\"optimize.jl\"]","category":"page"},{"location":"optimization/#GaussianProcesses.optimize!-Tuple{GPBase}","page":"Optimization","title":"GaussianProcesses.optimize!","text":"optimize!(gp::GPBase; kwargs...)\n\nOptimise the hyperparameters of Gaussian process gp based on type II maximum likelihood estimation. This function performs gradient based optimisation using the Optim pacakge to which the user is referred to for further details.\n\nKeyword arguments:\n\n* `domean::Bool`: Mean function hyperparameters should be optmized\n* `kern::Bool`: Kernel function hyperparameters should be optmized\n* `noise::Bool`: Observation noise hyperparameter should be optimized (GPE only)\n* `lik::Bool`: Likelihood hyperparameters should be optimized (GPA only)\n* `meanbounds`: [lowerbounds, upperbounds] for the mean hyperparameters\n* `kernbounds`: [lowerbounds, upperbounds] for the kernel hyperparameters\n* `noisebounds`: [lowerbound, upperbound] for the noise hyperparameter\n* `kwargs`: Keyword arguments for the optimize function from the Optim package\n\n\n\n\n\n","category":"method"},{"location":"poisson_regression/#Poisson-Regression-example-1","page":"Poisson Regression example","title":"Poisson Regression example","text":"","category":"section"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"Gaussian process models can be incredibly flexbile for modelling non-Gaussian data. One such example is in the case of count data mathbfy, which can be modelled with a Poisson model with a latent Gaussian process.","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"mathbfy    mathbff sim prod_i=1^n fraclambda_i^y_iexp-lambda_iy_i","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"where lambda_i=exp(f_i) and f_i is the latent Gaussian process.","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"#Load the package\nusing GaussianProcesses, Random, Distributions\n\n#Simulate the data\nRandom.seed!(203617)\nn = 20\nX = collect(range(-3,stop=3,length=n));\nf = 2*cos.(2*X);\nY = [rand(Poisson(exp.(f[i]))) for i in 1:n];\n\n#Plot the data using the Plots.jl package with the GR backend\nusing Plots\ngr()\nscatter(X,Y,leg=false, fmt=:png)","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"(Image: png)","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"#GP set-up\nk = Matern(3/2,0.0,0.0)   # Matern 3/2 kernel\nl = PoisLik()             # Poisson likelihood\ngpmc = GP(X, vec(Y), MeanZero(), k, l)\ngpvi = GP(X, vec(Y), MeanZero(), k, l)","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"GP Approximate object:\n  Dim = 1\n  Number of observations = 20\n  Mean function:\n    Type: MeanZero, Params: Float64[]\n  Kernel:\n    Type: Mat32Iso{Float64}, Params: [0.0, 0.0]\n  Likelihood:\n    Type: PoisLik, Params: Any[]\n  Input observations =\n[-3.0 -2.68421 … 2.68421 3.0]\n  Output observations = [3, 3, 1, 0, 0, 0, 0, 0, 3, 4, 7, 3, 1, 0, 0, 1, 0, 3, 4, 4]\n  Log-posterior = -65.397","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"set_priors!(gpmc.kernel,[Normal(-2.0,4.0),Normal(-2.0,4.0)])\n@time samples = mcmc(gpmc; nIter=10000);","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"Number of iterations = 10000, Thinning = 1, Burn-in = 1\nStep size = 0.100000, Average number of leapfrog steps = 10.029000\nNumber of function calls: 100291\nAcceptance rate: 0.801400\n  4.052686 seconds (24.20 M allocations: 2.026 GiB, 8.23% gc time)","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"#Sample predicted values\nxtest = range(minimum(gpmc.x),stop=maximum(gpmc.x),length=50);\nymean = [];\nfsamples = Array{Float64}(undef,size(samples,2), length(xtest));\nfor i in 1:size(samples,2)\n    set_params!(gpmc,samples[:,i])\n    update_target!(gpmc)\n    push!(ymean, predict_y(gpmc,xtest)[1])\n    fsamples[i,:] = rand(gpmc, xtest)\nend\n\n#Predictive plots\n\nq10 = [quantile(fsamples[:,i], 0.1) for i in 1:length(xtest)]\nq50 = [quantile(fsamples[:,i], 0.5) for i in 1:length(xtest)]\nq90 = [quantile(fsamples[:,i], 0.9) for i in 1:length(xtest)]\nplot(xtest,exp.(q50),ribbon=(exp.(q10), exp.(q90)),leg=true, fmt=:png, label=\"quantiles\")\nplot!(xtest,mean(ymean), label=\"posterior mean\")\nxx = range(-3,stop=3,length=1000);\nf_xx = 2*cos.(2*xx);\nplot!(xx, exp.(f_xx), label=\"truth\")\nscatter!(X,Y, label=\"data\")","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"(Image: png)","category":"page"},{"location":"poisson_regression/#Alternatives-to-MCMC-1","page":"Poisson Regression example","title":"Alternatives to MCMC","text":"","category":"section"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"As an alternative to MCMC, the practioner is also able to compute the approximate posterior using variational inference. This is done through the approach described in Khan et. al.. An approximate density q(mathbfx)=(2 pi)^-N  2mathbfSigma^-frac12 e^-frac12(mathbfx-boldsymbolmu)^top boldsymbolSigma^-1(mathbfx-boldsymbolmu) is used to replace the true posterior.","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"Syntactically, this can be found in a similar vein to mcmc by simply using the following statements.","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"@time Q = vi(gpvi);","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"Number of iterations = 1, Thinning = 1, Burn-in = 1\nStep size = 0.100000, Average number of leapfrog steps = 7.000000\nNumber of function calls: 8\nAcceptance rate: 0.000000\n  1.517399 seconds (1.09 M allocations: 471.231 MiB, 6.14% gc time)","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"nsamps = 500\nymean = [];\nvisamples = Array{Float64}(undef, nsamps, size(xtest, 1))\n\nfor i in 1:nsamps\n    visamples[i, :] = rand(gpvi, xtest, Q)\n    push!(ymean, predict_y(gpvi, xtest)[1])\nend\n\nq10 = [quantile(visamples[:, i], 0.1) for i in 1:length(xtest)]\nq50 = [quantile(visamples[:, i], 0.5) for i in 1:length(xtest)]\nq90 = [quantile(visamples[:, i], 0.9) for i in 1:length(xtest)]\nplot(xtest, exp.(q50), ribbon=(exp.(q10), exp.(q90)), leg=true, fmt=:png, label=\"quantiles\")\nplot!(xtest, mean(ymean), label=\"posterior mean\", w=2)\nxx = range(-3,stop=3,length=1000);\nf_xx = 2*cos.(2*xx);\nplot!(xx, exp.(f_xx), label=\"truth\")\nscatter!(X,Y, label=\"data\")","category":"page"},{"location":"poisson_regression/#","page":"Poisson Regression example","title":"Poisson Regression example","text":"(Image: png)","category":"page"},{"location":"plotting_gps/#Plotting-with-GaussianProcesses.jl-1","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"","category":"section"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"GaussianProcesses.jl provides recipes for plotting one and two-dimensional gaussian processes using the Plots.jl package. Plots.jl provides a general interface for plotting with several different backends including PyPlot, Plotly and GR.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Currently plotting is only supported for GPE objects i.e. Gaussian processes with Gaussian likelihood functions.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Plots.jl is not a dependency of GaussianProcesses.jl as the plotting functionality is implemented through the skeleton package RecipesBase.jl. To plot a GP object one must therefore first install and load the Plots.jl package:","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"using Plots\npyplot()   # Optionally select a plotting backend","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Plots.PyPlotBackend()","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Plotting backends such as PyPlot.jl, may also have to be installed manually.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Once a GPE object is constructed, the plot command can be used to plot the mean function of the Gaussian process. The user can modify attributes of the plot such as the axis labels with the usual optional keyword arguments:","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"using GaussianProcesses\n\n# Generate random data for Gaussian process\nx = 2π * rand(10)\ny = sin.(x) + 0.5*rand(10)\n\n# Set-up mean and kernel\nse = SE(0.0, 0.0)\nm = MeanZero()\n\n# Construct and plot GP\ngp = GP(x,y,m,se)\nplot(gp;  xlabel=\"gp.x\", ylabel=\"gp.y\", title=\"Gaussian process\", legend=false, fmt=:png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"(Image: png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"By default, the plot command plots the with a ribbon representing 95% confidence interval on the output observations, and with the observation points. These aspects be controlled with the following special additional keyword arguments:","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"β (default=0.95) : level of confidence band plotted with mean (set to 0.0 for no band)\nobsv (default=true) : plot observations with mean function","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Note that not all plotting backends are able to plot the confidence band. Specifically, only those which support the ribbon attribute can plot the confidence band. See here for tables detailing which backends support which series types and attributes.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Plots.jl allows the user to modify and combine plots in complicated ways. The following plots a fitted Gaussian process with sampled processes.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"x = 0:0.1:2π\nplot(gp; obsv=false)\noptimize!(gp)\nplot(gp; obsv=false, label=\"GP posterior mean\", fmt=:png)\nsamples = rand(gp, x, 5)\nplot!(x, samples)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"(Image: png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Two-dimensional Gaussian processes are plotted in the same way. In this case, the plot command has the following special keyword:","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"var (default = false): plot variance of Gaussian process instead of the mean","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"By default, the plot command will produce a contour plot for a two-dimensional process.","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"# Simulate data for 2D Gaussian process\nX = 2π*rand(2, 10)\ny = sin.(X[1,:]) .* cos.(X[2,:]) + 0.5*rand(10)\ngp2 = GP(X,y,m,se)\n# Plot mean and variance\np1 = plot(gp2; title=\"Mean of GP\")\np2 = plot(gp2; var=true, title=\"Variance of GP\", fill=true)\nplot(p1, p2; fmt=:png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"(Image: png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"Plots.jl has recipes for different types of two-dimensional plots including heatmaps, surfaces, contours and wireframes. The user can select the type of plot using the seriestype keyword, or the appropriate function name:","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"gr() # use GR backend to allow wireframe plot\np1 = contour(gp2)\np2 = surface(gp2)\np3 = heatmap(gp2)\np4 = wireframe(gp2)\nplot(p1, p2, p3, p4; fmt=:png)","category":"page"},{"location":"plotting_gps/#","page":"Plotting with GaussianProcesses.jl","title":"Plotting with GaussianProcesses.jl","text":"(Image: png)","category":"page"},{"location":"gp/#Gaussian-Processes-1","page":"Gaussian Processes","title":"Gaussian Processes","text":"","category":"section"},{"location":"gp/#","page":"Gaussian Processes","title":"Gaussian Processes","text":"Modules = [GaussianProcesses]\nPages = [\"GP.jl\", \"GPE.jl\", \"GPA.jl\", \"GPEelastic.jl\", \"common.jl\", \"mcmc.jl\", \"utils.jl\"]","category":"page"},{"location":"gp/#GaussianProcesses.predict_f-Tuple{GPBase,AbstractArray{T,2} where T}","page":"Gaussian Processes","title":"GaussianProcesses.predict_f","text":"predict_f(gp::GPBase, X::Matrix{Float64}[]; full_cov::Bool = false)\n\nReturn posterior mean and variance of the Gaussian Process gp at specfic points which are given as columns of matrix X. If full_cov is true, the full covariance matrix is returned instead of only variances.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.GPE","page":"Gaussian Processes","title":"GaussianProcesses.GPE","text":"GPE(x, y, mean, kernel[, logNoise])\n\nFit a Gaussian process to a set of training points. The Gaussian process is defined in terms of its user-defined mean and covariance (kernel) functions. As a default it is assumed that the observations are noise free.\n\nArguments:\n\nx::AbstractVecOrMat{Float64}: Input observations\ny::AbstractVector{Float64}: Output observations\nmean::Mean: Mean function\nkernel::Kernel: Covariance function\nlogNoise::Float64: Natural logarithm of the standard deviation for the observation noise. The default is -2.0, which is equivalent to assuming no observation noise.\n\n\n\n\n\n","category":"type"},{"location":"gp/#GaussianProcesses.GPE-Tuple{}","page":"Gaussian Processes","title":"GaussianProcesses.GPE","text":"GPE(; mean::Mean = MeanZero(), kernel::Kernel = SE(0.0, 0.0), logNoise::AbstractFloat = -2.0)\n\nConstruct a GPE object without observations.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.GP","page":"Gaussian Processes","title":"GaussianProcesses.GP","text":"GP(x, y, mean::Mean, kernel::Kernel[, logNoise::AbstractFloat=-2.0])\n\nFit a Gaussian process that is defined by its mean, its kernel, and the logarithm logNoise of the standard deviation of its observation noise to a set of training points x and y.\n\nSee also: GPE\n\n\n\n\n\n","category":"function"},{"location":"gp/#GaussianProcesses.update_target!-Tuple{GPE}","page":"Gaussian Processes","title":"GaussianProcesses.update_target!","text":"update_target!(gp::GPE, ...)\n\nUpdate the log-posterior\n\nlog p(θ  y)  log p(y  θ) +  log p(θ)\n\nof a Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.GPA-Tuple{AbstractArray{T,2} where T,AbstractArray{#s152,1} where #s152<:Real,GaussianProcesses.Mean,Kernel,Likelihood}","page":"Gaussian Processes","title":"GaussianProcesses.GPA","text":"GPA(x, y, mean, kernel, lik)\n\nFit a Gaussian process to a set of training points. The Gaussian process with non-Gaussian observations is defined in terms of its user-defined likelihood function, mean and covaiance (kernel) functions.\n\nThe non-Gaussian likelihood is handled by an approximate method (e.g. Monte Carlo). The latent function values are represented by centered (whitened) variables f(x) = m(x) + Lv where v  N(0 I) and LLᵀ = K_θ.\n\nArguments:\n\nx::AbstractVecOrMat{Float64}: Input observations\ny::AbstractVector{<:Real}: Output observations\nmean::Mean: Mean function\nkernel::Kernel: Covariance function\nlik::Likelihood: Likelihood function\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.GP-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},AbstractArray{#s152,1} where #s152<:Real,GaussianProcesses.Mean,Kernel,Likelihood}","page":"Gaussian Processes","title":"GaussianProcesses.GP","text":"GP(x, y, mean::Mean, kernel::Kernel, lik::Likelihood)\n\nFit a Gaussian process that is defined by its mean, its kernel, and its likelihood function lik to a set of training points x and y.\n\nSee also: GPA\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_target!-Tuple{GPA}","page":"Gaussian Processes","title":"GaussianProcesses.update_target!","text":"update_target!(gp::GPA, ...)\n\nUpdate the log-posterior\n\nlog p(θ v  y)  log p(y  v θ) + log p(v) + log p(θ)\n\nof a Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.ess-Tuple{GPE}","page":"Gaussian Processes","title":"GaussianProcesses.ess","text":"ess(gp::GPBase; kwargs...)\n\nSample GP hyperparameters using the elliptical slice sampling algorithm described in,\n\nMurray, Iain, Ryan P. Adams, and David JC MacKay. \"Elliptical slice sampling.\"  Journal of Machine Learning Research 9 (2010): 541-548.\n\nRequires hyperparameter priors to be Gaussian.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.mcmc-Tuple{GPBase}","page":"Gaussian Processes","title":"GaussianProcesses.mcmc","text":"mcmc(gp::GPBase; kwargs...)\n\nRuns Hamiltonian Monte Carlo algorithm for estimating the hyperparameters of Gaussian process GPE and the latent function in the case of GPA.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.CovarianceStrategy","page":"Gaussian Processes","title":"GaussianProcesses.CovarianceStrategy","text":"The abstract CovarianceStrategy type is for types that control how\nthe covariance matrices and their positive definite representation\nare obtained or approximated. See SparseStrategy for examples.\n\n\n\n\n\n","category":"type"},{"location":"gp/#GaussianProcesses.make_posdef!-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T}","page":"Gaussian Processes","title":"GaussianProcesses.make_posdef!","text":"make_posdef!(m::Matrix{Float64}, chol_factors::Matrix{Float64})\n\nTry to encode covariance matrix m as a positive definite matrix. The chol_factors matrix is recycled to store the cholesky decomposition, so as to reduce the number of memory allocations.\n\nSometimes covariance matrices of Gaussian processes are positive definite mathematically but have negative eigenvalues numerically. To resolve this issue, small weights are added to the diagonal (and hereby all eigenvalues are raised by that amount mathematically) until all eigenvalues are positive numerically.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.predictMVN-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T,AbstractArray{T,1} where T,Kernel,GaussianProcesses.Mean,AbstractArray{T,1} where T,GaussianProcesses.CovarianceStrategy,PDMats.AbstractPDMat}","page":"Gaussian Processes","title":"GaussianProcesses.predictMVN","text":"    predictMVN(xpred::AbstractMatrix, xtrain::AbstractMatrix, ytrain::AbstractVector,\n               kernel::Kernel, meanf::Mean, alpha::AbstractVector,\n               covstrat::CovarianceStrategy, Ktrain::AbstractPDMat)\n\nCompute predictions using the standard multivariate normal conditional distribution formulae.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.AbstractGradientPrecompute","page":"Gaussian Processes","title":"GaussianProcesses.AbstractGradientPrecompute","text":"AbstractGradientPrecompute types hold results of     pre-computations of kernel gradients.\n\n\n\n\n\n","category":"type"},{"location":"gp/#GaussianProcesses.dmll_kern!-Tuple{AbstractArray{T,1} where T,Kernel,AbstractArray{T,2} where T,GaussianProcesses.KernelData,Array{Float64,2},GaussianProcesses.CovarianceStrategy}","page":"Gaussian Processes","title":"GaussianProcesses.dmll_kern!","text":"dmll_kern!((dmll::AbstractVector, k::Kernel, X::AbstractMatrix, data::KernelData, ααinvcKI::AbstractMatrix))\n\nDerivative of the marginal log likelihood log p(Y|θ) with respect to the kernel hyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.fit!-Union{Tuple{Y}, Tuple{X}, Tuple{GPE{X,Y,M,K,CS,D,P} where P<:AbstractPDMat where D<:KernelData where CS<:CovarianceStrategy where K<:Kernel where M<:Mean,X,Y}} where Y where X","page":"Gaussian Processes","title":"GaussianProcesses.fit!","text":"fit!(gp::GPE{X,Y}, x::X, y::Y)\n\nFit Gaussian process GPE to a training data set consisting of input observations x and output observations y.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.get_ααinvcKI!-Tuple{AbstractArray{T,2} where T,PDMats.AbstractPDMat,Array{T,1} where T}","page":"Gaussian Processes","title":"GaussianProcesses.get_ααinvcKI!","text":"get_ααinvcKI!(ααinvcKI::Matrix{Float64}, cK::AbstractPDMat, α::Vector)\n\nWrite ααᵀ - cK⁻¹ to ααinvcKI avoiding any memory allocation, where cK and α are the covariance matrix and the alpha vector of a Gaussian process, respectively. Hereby α is defined as cK⁻¹ (Y - μ).\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.initialise_target!-Tuple{GPE}","page":"Gaussian Processes","title":"GaussianProcesses.initialise_target!","text":"initialise_target!(gp::GPE)\n\nInitialise the log-posterior\n\nlog p(θ  y)  log p(y  θ) +  log p(θ)\n\nof a Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_cK!-Tuple{GPE}","page":"Gaussian Processes","title":"GaussianProcesses.update_cK!","text":"update_cK!(gp::GPE)\n\nUpdate the covariance matrix and its Cholesky decomposition of Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_dmll!-Tuple{GPE,GaussianProcesses.AbstractGradientPrecompute}","page":"Gaussian Processes","title":"GaussianProcesses.update_dmll!","text":" update_dmll!(gp::GPE, ...)\n\nUpdate the gradient of the marginal log-likelihood of Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_mll!-Tuple{GPE}","page":"Gaussian Processes","title":"GaussianProcesses.update_mll!","text":"update_mll!(gp::GPE)\n\nModification of initialise_target! that reuses existing matrices to avoid unnecessary memory allocations, which speeds things up significantly.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_mll_and_dmll!-Tuple{GPE,GaussianProcesses.AbstractGradientPrecompute}","page":"Gaussian Processes","title":"GaussianProcesses.update_mll_and_dmll!","text":"update_mll_and_dmll!(gp::GPE, ...)\n\nUpdate the gradient of the marginal log-likelihood of a Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_target_and_dtarget!-Tuple{GPE,GaussianProcesses.AbstractGradientPrecompute}","page":"Gaussian Processes","title":"GaussianProcesses.update_target_and_dtarget!","text":"update_target_and_dtarget!(gp::GPE, ...)\n\nUpdate the log-posterior\n\nlog p(θ  y)  log p(y  θ) +  log p(θ)\n\nof a Gaussian process gp and its derivative.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.initialise_ll!-Tuple{GPA}","page":"Gaussian Processes","title":"GaussianProcesses.initialise_ll!","text":"initialise_ll!(gp::GPA)\n\nInitialise the log-likelihood of Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.initialise_target!-Tuple{GPA}","page":"Gaussian Processes","title":"GaussianProcesses.initialise_target!","text":"initialise_target!(gp::GPA)\n\nInitialise the log-posterior\n\nlog p(θ v  y)  log p(y  v θ) + log p(v) + log p(θ)\n\nof a Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_cK!-Tuple{GPA}","page":"Gaussian Processes","title":"GaussianProcesses.update_cK!","text":"update_cK!(gp::GPA)\n\nUpdate the covariance matrix and its Cholesky decomposition of Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_dll!-Tuple{GPA,GaussianProcesses.AbstractGradientPrecompute}","page":"Gaussian Processes","title":"GaussianProcesses.update_dll!","text":" update_dll!(gp::GPA, ...)\n\nUpdate the gradient of the log-likelihood of Gaussian process gp.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.update_target_and_dtarget!-Tuple{GPA,GaussianProcesses.AbstractGradientPrecompute}","page":"Gaussian Processes","title":"GaussianProcesses.update_target_and_dtarget!","text":"update_target_and_dtarget!(gp::GPA, ...)\n\nUpdate the log-posterior\n\nlog p(θ v  y)  log p(y  v θ) + log p(v) + log p(θ)\n\nof a Gaussian process gp and its derivative.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.composite_param_names-Tuple{Any,Any}","page":"Gaussian Processes","title":"GaussianProcesses.composite_param_names","text":"composite_param_names(objects, prefix)\n\nCall get_param_names on each element of objects and prefix the returned name of the element at index i with prefix * i * '_'.\n\nExamples\n\njulia> GaussianProcesses.get_param_names(ProdKernel(Mat12Iso(1/2, 1/2), SEArd([0.0, 1.0], 0.0)))\n5-element Array{Symbol,1}:\n :pk1_ll\n :pk1_lσ\n :pk2_ll_1\n :pk2_ll_2\n :pk2_lσ\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.map_column_pairs!-Tuple{AbstractArray{T,2} where T,Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}","page":"Gaussian Processes","title":"GaussianProcesses.map_column_pairs!","text":"map_column_pairs!(D::Matrix{Float64}, f, X::Matrix{Float64}[, Y::Matrix{Float64} = X])\n\nLike map_column_pairs, but stores the result in D rather than a new matrix.\n\n\n\n\n\n","category":"method"},{"location":"gp/#GaussianProcesses.map_column_pairs-Tuple{Any,AbstractArray{T,2} where T,AbstractArray{T,2} where T}","page":"Gaussian Processes","title":"GaussianProcesses.map_column_pairs","text":"map_column_pairs(f, X::Matrix{Float64}[, Y::Matrix{Float64} = X])\n\nCreate a matrix by applying function f to each pair of columns of input matrices X and Y.\n\n\n\n\n\n","category":"method"},{"location":"mean/#Means-1","page":"Means","title":"Means","text":"","category":"section"},{"location":"mean/#","page":"Means","title":"Means","text":"Modules = [GaussianProcesses]\nPages = readdir(joinpath(\"..\", \"src\", \"means\"))","category":"page"},{"location":"mean/#GaussianProcesses.MeanConst","page":"Means","title":"GaussianProcesses.MeanConst","text":"MeanConst <: Mean\n\nConstant mean function\n\nm(x) = β\n\nwith constant β.\n\n\n\n\n\n","category":"type"},{"location":"mean/#GaussianProcesses.MeanLin","page":"Means","title":"GaussianProcesses.MeanLin","text":"MeanLin <: Mean\n\nLinear mean function\n\nm(x) = xᵀβ\n\nwith linear coefficients β.\n\n\n\n\n\n","category":"type"},{"location":"mean/#GaussianProcesses.MeanPeriodic","page":"Means","title":"GaussianProcesses.MeanPeriodic","text":"MeanPeriodic <: Mean\n\nPeriodic mean function\n\nm(x) = acos(2πxp) + bsin(2πxp)\n\nwith polynomial coefficients βᵢⱼ of shape d  D where d is the dimension of observations and D is the degree of the polynomial.\n\n\n\n\n\n","category":"type"},{"location":"mean/#GaussianProcesses.MeanPoly","page":"Means","title":"GaussianProcesses.MeanPoly","text":"MeanPoly <: Mean\n\nPolynomial mean function\n\nm(x) = ᵢⱼ βᵢⱼxᵢʲ\n\nwith polynomial coefficients βᵢⱼ of shape d  D where d is the dimension of observations and D is the degree of the polynomial.\n\n\n\n\n\n","category":"type"},{"location":"mean/#GaussianProcesses.MeanZero","page":"Means","title":"GaussianProcesses.MeanZero","text":"MeanZero <: Mean\n\nZero mean function\n\nm(x) = 0\n\n\n\n\n\n","category":"type"},{"location":"sparse/#Sparse-GPs-1","page":"Sparse GPs","title":"Sparse GPs","text":"","category":"section"},{"location":"sparse/#","page":"Sparse GPs","title":"Sparse GPs","text":"Modules = [GaussianProcesses]\nPages = readdir(joinpath(\"..\", \"src\", \"sparse\"))","category":"page"},{"location":"sparse/#GaussianProcesses.predict_f-Tuple{GPBase,AbstractArray{T,2} where T,Array{#s270,1} where #s270<:AbstractArray{Int64,1}}","page":"Sparse GPs","title":"GaussianProcesses.predict_f","text":"predict_f(gp::GPBase, X::Matrix{Float64}[; full_cov::Bool = false])\n\nReturn posterior mean and variance of the Gaussian Process gp at specfic points which are given as columns of matrix X. If full_cov is true, the full covariance matrix is returned instead of only variances.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.DeterminTrainCondStrat","page":"Sparse GPs","title":"GaussianProcesses.DeterminTrainCondStrat","text":"Deterministic Training Conditional (DTC) covariance strategy.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#GaussianProcesses.predictMVN-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T,AbstractArray{T,1} where T,Kernel,GaussianProcesses.Mean,AbstractArray{T,1} where T,GaussianProcesses.DeterminTrainCondStrat,PDMats.AbstractPDMat}","page":"Sparse GPs","title":"GaussianProcesses.predictMVN","text":"Deterministic Training Conditional (DTC) multivariate normal predictions.\n\nSee Quiñonero-Candela and Rasmussen 2005, equations 20b.\n    μ_DTC = μ_SoR\n    Σ_DTC = Σxx - Qxx + Σ_SoR\n\nwhere μ_DTC and Σ_DTC are the predictive mean and covariance\nfunctions for the Subset of Regressors approximation.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.FullScaleApproxStrat","page":"Sparse GPs","title":"GaussianProcesses.FullScaleApproxStrat","text":"Fully Independent Training Conditional (FSA) covariance strategy.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#GaussianProcesses.FullScalePDMat","page":"Sparse GPs","title":"GaussianProcesses.FullScalePDMat","text":"Positive Definite Matrix for Full Scale Approximation.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#Base.:\\-Tuple{GaussianProcesses.FullScalePDMat,Any}","page":"Sparse GPs","title":"Base.:\\","text":"We have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\nwhere Λ is a diagonal matrix (here given by σ²I + diag(Kff - Qff))\nThe Woodbury matrix identity gives\n    (A+UCV)⁻¹ = A⁻¹ - A⁻¹ U(C⁻¹ + V A⁻¹ U)⁻¹ V A⁻¹\nwhich we use here with\n    A ← Λ\n    U = Kuf'\n    V = Kuf\n    C = Kuu⁻¹\nwhich gives\n    Σ⁻¹ = Λ⁻¹ - Λ⁻¹ Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf Λ⁻¹\n        = Λ⁻¹ - Λ⁻¹ Kuf'(        ΣQR       )⁻¹ Kuf Λ⁻¹\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_kern!-Tuple{AbstractArray{T,1} where T,Kernel,AbstractArray{T,2} where T,PDMats.AbstractPDMat,GaussianProcesses.SparseKernelData,AbstractArray{T,1} where T,Any,Any,Any,Any,Any,Any,Any,GaussianProcesses.FullScaleApproxStrat}","page":"Sparse GPs","title":"GaussianProcesses.dmll_kern!","text":"dmll_kern!(dmll::AbstractVector, kernel::Kernel, X::AbstractMatrix, cK::AbstractPDMat, kerneldata::KernelData,                     alpha::AbstractVector, Kuu, Kuf, Kuu⁻¹Kuf, Kuu⁻¹KufΣ⁻¹y, Σ⁻¹Kfu, ∂Kuu, ∂Kuf,                     covstrat::FullScaleApproxStrat) Derivative of the log likelihood under the Fully Independent Training Conditional (fsa) approximation.\n\nHelpful reference: Vanhatalo, Jarno, and Aki Vehtari.                    \"Sparse log Gaussian processes via MCMC for spatial epidemiology.\"                    In Gaussian processes in practice, pp. 73-89. 2007.\n\nGenerally, for a multivariate normal with zero mean     ∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)                     ╰───────────────╯     ╰──────────╯                            V                 T\n\nwhere Σ = Kff + σ²I.\n\nNotation: f is the observations, u is the inducing points.           ∂X stands for ∂X/∂θ, where θ is the kernel hyperparameters.\n\nIn the case of the FSA approximation, we have     Σ = Λ + Qff     The second component gives     ∂(Qff) = ∂(Kfu Kuu⁻¹ Kuf)     which is used by the gradient function for the subset of regressors approximation,     and so I don't repeat it here.\n\nfor ∂Λ we have (with `i` indexing each block)\nΛi = Ki - Qi + σ²I\n   = Ki - Kui' Kuu⁻¹ Kui + σ²\n∂Λi = ∂Ki - Kui' ∂(Kuu⁻¹) Kui - 2 ∂Kui' Kuu⁻¹ Kui\n    = ∂Ki + Kui' Kuu⁻¹ ∂(Kuu) Kuu⁻¹ Kui - 2 ∂Kui' Kuu⁻¹ Kui\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_noise-Tuple{GPE,GaussianProcesses.SoRPrecompute,GaussianProcesses.FullScaleApproxStrat}","page":"Sparse GPs","title":"GaussianProcesses.dmll_noise","text":"dmll_noise(gp::GPE, precomp::SoRPrecompute, covstrat::FullScaleApproxStrat)\n\n∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)\n\n∂Σ = I for derivative wrt σ², so ∂logp(Y|θ) = 1/2 y' Σ⁻¹ Σ⁻¹ y - 1/2 tr(Σ⁻¹)             = 1/2[ dot(α,α) - tr(Σ⁻¹) ]\n\nWe have:     Σ⁻¹ = Λ⁻¹ - Λ⁻² Kuf'(        ΣQR       )⁻¹ Kuf Use the identity tr(A'A) = dot(A,A) to get:     tr(Σ⁻¹) = tr(Λ⁻¹) - dot(Lk, Lk) . where     Lk ≡ ΣQR^(-1/2) Kuf Λ⁻¹\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.predictMVN-Tuple{AbstractArray{T,2} where T,Array{#s270,1} where #s270<:AbstractArray{Int64,1},AbstractArray{T,2} where T,AbstractArray{T,1} where T,Kernel,GaussianProcesses.Mean,AbstractArray{T,1} where T,GaussianProcesses.FullScaleApproxStrat,GaussianProcesses.FullScalePDMat}","page":"Sparse GPs","title":"GaussianProcesses.predictMVN","text":"predictMVN(xpred::AbstractMatrix, xtrain::AbstractMatrix, ytrain::AbstractVector,                     kernel::Kernel, meanf::Mean, logNoise::Real,                     alpha::AbstractVector,                     covstrat::FullScaleApproxStrat, Ktrain::FullScalePDMat)     See Quiñonero-Candela and Rasmussen 2005, equations 24b.     Some derivations can be found below that are not spelled out in the paper.\n\nNotation: Qab = Kau Kuu⁻¹ Kub\n          ΣQR = Kuu + σ⁻² Kuf Kuf'\n\n          x: prediction (test) locations\n          f: training (observed) locations\n          u: inducing point locations\n\nWe have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\nBy Woodbury\n    Σ⁻¹ = Λ⁻¹ - Λ⁻² Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf\n        = Λ⁻¹ - Λ⁻² Kuf'(        ΣQR       )⁻¹ Kuf\n\nThe predictive mean can be derived (assuming zero mean function for simplicity)\nμ = (Qxf+Λxf) (Qff + Λff)⁻¹ y\n  = Qxf (Qff + Λff)⁻¹ y   + Λxf (Qff + Λff)⁻¹ y\n    ╰─────────────────╯\n        same as FITC\n  = Kxu ΣQR⁻¹ Kuf Λff⁻¹ y + Λxf (Qff + Λff)⁻¹ y\n       ╰────────────────╯       ╰─────────────╯\n          ≡ alpha_u                ≡ alpha\n\nSimilarly for the posterior predictive covariance:\nΣ = Σxx - (Qxf+Λxf) (Qff + Λff)⁻¹ (Qxf+Λxf)'\n  = Σxx - Kxu ΣQR⁻¹ Kuf Λ⁻¹ Qxf'                # substituting result from μ\n  = Σxx - Kxu ΣQR⁻¹  Kuf Λ⁻¹ Kfu Kuu⁻¹ Kux      # definition of Qxf\n  = Σxx - Kxu ΣQR⁻¹ (ΣQR - Kuu) Kuu⁻¹ Kux       # using definition of ΣQR\n  = Σxx - Kxu Kuu⁻¹ Kux + Kxu ΣQR⁻¹ Kux         # expanding\n  = Σxx - Qxx           + Kxu ΣQR⁻¹ Kux         # definition of Qxx\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.trinv-Tuple{GaussianProcesses.BlockDiagPDMat}","page":"Sparse GPs","title":"GaussianProcesses.trinv","text":"trinv(a::BlockDiagPDMat)\n\nTrace of the inverse of a block diagonal positive definite matrix.\n\nThis is obtained as the sum of the traces of the inverse of each block.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.trinv-Tuple{PDMats.AbstractPDMat}","page":"Sparse GPs","title":"GaussianProcesses.trinv","text":"trinv(pd::AbstractPDMat)\n\nTrace of the inverse of a positive definite matrix.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.trinvAB-Tuple{GaussianProcesses.FullScalePDMat,LinearAlgebra.Diagonal}","page":"Sparse GPs","title":"GaussianProcesses.trinvAB","text":"trinvAB(A::FullScalePDMat, B::Diagonal)\n\nComputes tr(A⁻¹ B) efficiently under the FSA approximation:\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\n\nDerivation:\n    tr(Σ⁻¹ B) = tr[ (Λ⁻¹ - Λ⁻² Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf) B ]\n              = tr(Λ⁻¹ B) - tr( Λ⁻¹ Kuf' ΣQR⁻¹ Kuf Λ⁻¹ B )\n              = tr(Λ⁻¹ B) - tr( Λ⁻¹ Kuf' ΣQR^(-1/2) ΣQR^(-1/2) Kuf Λ⁻¹ B )\n                                                    ╰────────────────╯\n                                                        ≡ L'\n              = tr(Λ⁻¹ B) - tr(L*L'*B)\n              = tr(Λ⁻¹ B) - dot(L, B*L)\n\nSee also: [``](@ref).\n\n\n\n\n\n","category":"method"},{"location":"sparse/#LinearAlgebra.logdet-Tuple{GaussianProcesses.FullScalePDMat}","page":"Sparse GPs","title":"LinearAlgebra.logdet","text":"The matrix determinant lemma states that\n    logdet(A+UWV') = logdet(W⁻¹ + V'A⁻¹U) + logdet(W) + logdet(A)\nSo for\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\n    logdet(Σ) = logdet(Kuu + Kuf Λ⁻¹ Kuf')       + logdet(Kuu⁻¹) + logdet(Λ)\n              = logdet(        ΣQR             ) - logdet(Kuu)   + logdet(Λ)\n\n\n\n\n\n","category":"method"},{"location":"sparse/#LinearAlgebra.tr-Tuple{GaussianProcesses.FullScalePDMat}","page":"Sparse GPs","title":"LinearAlgebra.tr","text":"tr(a::FullScalePDMat)\n\nTrace of the FSA approximation to the covariance matrix:\n\ntr(Σ) = tr(Kuf' Kuu⁻¹ Kuf + Λ)\n      = tr(Kuf' Kuu⁻¹ Kuf) + tr(Λ)\n      = tr(Kuf' Kuu^{-1/2} Kuu^{-1/2} Kuf) + tr(Λ)\n                          ╰──────────────╯\n                             ≡  Lk\n      = dot(Lk, Lk) + sum(diag(Λ))\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.FullyIndepPDMat","page":"Sparse GPs","title":"GaussianProcesses.FullyIndepPDMat","text":"Positive Definite Matrix for Fully Independent Training Conditional approximation.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#GaussianProcesses.FullyIndepStrat","page":"Sparse GPs","title":"GaussianProcesses.FullyIndepStrat","text":"Fully Independent Training Conditional (FITC) covariance strategy.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#Base.:\\-Tuple{GaussianProcesses.FullyIndepPDMat,Any}","page":"Sparse GPs","title":"Base.:\\","text":"We have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\nwhere Λ is a diagonal matrix (here given by σ²I + diag(Kff - Qff))\nThe Woodbury matrix identity gives\n    (A+UCV)⁻¹ = A⁻¹ - A⁻¹ U(C⁻¹ + V A⁻¹ U)⁻¹ V A⁻¹\nwhich we use here with\n    A ← Λ\n    U = Kuf'\n    V = Kuf\n    C = Kuu⁻¹\nwhich gives\n    Σ⁻¹ = Λ⁻¹ - Λ⁻² Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf\n        = Λ⁻¹ - Λ⁻² Kuf'(        ΣQR       )⁻¹ Kuf\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_kern!-Tuple{AbstractArray{T,1} where T,Kernel,AbstractArray{T,2} where T,PDMats.AbstractPDMat,GaussianProcesses.SparseKernelData,AbstractArray{T,1} where T,Any,Any,Any,Any,Any,Any,Any,GaussianProcesses.FullyIndepStrat}","page":"Sparse GPs","title":"GaussianProcesses.dmll_kern!","text":"dmll_kern!(dmll::AbstractVector, kernel::Kernel, X::AbstractMatrix, cK::AbstractPDMat, kerneldata::KernelData,                     alpha::AbstractVector, Kuu, Kuf, Kuu⁻¹Kuf, Kuu⁻¹KufΣ⁻¹y, Σ⁻¹Kfu, ∂Kuu, ∂Kuf,                     covstrat::FullyIndepStrat) Derivative of the log likelihood under the Fully Independent Training Conditional (FITC) approximation.\n\nHelpful reference: Vanhatalo, Jarno, and Aki Vehtari.                    \"Sparse log Gaussian processes via MCMC for spatial epidemiology.\"                    In Gaussian processes in practice, pp. 73-89. 2007.\n\nGenerally, for a multivariate normal with zero mean     ∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)                     ╰───────────────╯     ╰──────────╯                            V                 T\n\nwhere Σ = Kff + σ²I.\n\nNotation: f is the observations, u is the inducing points.           ∂X stands for ∂X/∂θ, where θ is the kernel hyperparameters.\n\nIn the case of the FITC approximation, we have     Σ = Λ + Qff     The second component gives     ∂(Qff) = ∂(Kfu Kuu⁻¹ Kuf)     which is used by the gradient function for the subset of regressors approximation,     and so I don't repeat it here.\n\nthe ith element of diag(Kff-Qff) is\nΛi = Kii - Qii + σ²\n   = Kii - Kui' Kuu⁻¹ Kui + σ²\n∂Λi = ∂Kii - Kui' ∂(Kuu⁻¹) Kui - 2 ∂Kui' Kuu⁻¹ Kui\n    = ∂Kii + Kui' Kuu⁻¹ ∂(Kuu) Kuu⁻¹ Kui - 2 ∂Kui' Kuu⁻¹ Kui\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_noise-Tuple{GPE,GaussianProcesses.SoRPrecompute,GaussianProcesses.FullyIndepStrat}","page":"Sparse GPs","title":"GaussianProcesses.dmll_noise","text":"dmll_noise(gp::GPE, precomp::SoRPrecompute, covstrat::FullyIndepStrat)\n\n∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)\n\n∂Σ = I for derivative wrt σ², so ∂logp(Y|θ) = 1/2 y' Σ⁻¹ Σ⁻¹ y - 1/2 tr(Σ⁻¹)             = 1/2[ dot(α,α) - tr(Σ⁻¹) ]\n\nWe have:     Σ⁻¹ = Λ⁻¹ - Λ⁻² Kuf'(        ΣQR       )⁻¹ Kuf Use the identity tr(A'A) = dot(A,A) to get:     tr(Σ⁻¹) = tr(Λ⁻¹) - dot(Lk, Lk) . where     Lk ≡ ΣQR^(-1/2) Kuf Λ⁻¹\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.predictMVN-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T,AbstractArray{T,1} where T,Kernel,GaussianProcesses.Mean,AbstractArray{T,1} where T,GaussianProcesses.FullyIndepStrat,GaussianProcesses.FullyIndepPDMat}","page":"Sparse GPs","title":"GaussianProcesses.predictMVN","text":"predictMVN(xpred::AbstractMatrix, xtrain::AbstractMatrix, ytrain::AbstractVector,                     kernel::Kernel, meanf::Mean, logNoise::Real,                     alpha::AbstractVector,                     covstrat::FullyIndepStrat, Ktrain::FullyIndepPDMat)     See Quiñonero-Candela and Rasmussen 2005, equations 24b.     Some derivations can be found below that are not spelled out in the paper.\n\nNotation: Qab = Kau Kuu⁻¹ Kub\n          ΣQR = Kuu + σ⁻² Kuf Kuf'\n\n          x: prediction (test) locations\n          f: training (observed) locations\n          u: inducing point locations\n\nWe have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\nBy Woodbury\n    Σ⁻¹ = Λ⁻¹ - Λ⁻² Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf\n        = Λ⁻¹ - Λ⁻² Kuf'(        ΣQR       )⁻¹ Kuf\n\nThe predictive mean can be derived (assuming zero mean function for simplicity)\nμ = Qxf (Qff + Λ)⁻¹ y\n  = Kxu Kuu⁻¹ Kuf [Λ⁻¹ - Λ⁻² Kuf' ΣQR⁻¹ Kuf] y   # see Woodbury formula above.\n  = Kxu Kuu⁻¹ [ΣQR - Kuf Λ⁻¹ Kfu] ΣQR⁻¹ Kuf Λ⁻¹ y # factoring out common terms\n  = Kxu Kuu⁻¹ [Kuu] ΣQR⁻¹ Kuf Λ⁻¹ y               # using definition of ΣQR\n  = Kxu ΣQR⁻¹ Kuf Λ⁻¹ y                           # matches equation 16b\n\nSimilarly for the posterior predictive covariance:\nΣ = Σxx - Qxf (Qff + Λ)⁻¹ Qxf'\n  = Σxx - Kxu ΣQR⁻¹ Kuf Λ⁻¹ Qxf'                # substituting result from μ\n  = Σxx - Kxu ΣQR⁻¹  Kuf Λ⁻¹ Kfu Kuu⁻¹ Kux      # definition of Qxf\n  = Σxx - Kxu ΣQR⁻¹ (ΣQR - Kuu) Kuu⁻¹ Kux       # using definition of ΣQR\n  = Σxx - Kxu Kuu⁻¹ Kux + Kxu ΣQR⁻¹ Kux         # expanding\n  = Σxx - Qxx           + Kxu ΣQR⁻¹ Kux         # definition of Qxx\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.trinvAB-Tuple{GaussianProcesses.FullyIndepPDMat,LinearAlgebra.Diagonal}","page":"Sparse GPs","title":"GaussianProcesses.trinvAB","text":"trinvAB(A::FullyIndepPDMat, B::Diagonal)\n\nComputes tr(A⁻¹ B) efficiently under the FITC approximation:\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\n\nDerivation:\n    tr(Σ⁻¹ B) = tr[ (Λ⁻¹ - Λ⁻² Kuf'(Kuu + Kuf Λ⁻¹ Kuf')⁻¹ Kuf) B ]\n              = tr(Λ⁻¹ B) - tr( Λ⁻¹ Kuf' ΣQR⁻¹ Kuf Λ⁻¹ B )\n              = tr(Λ⁻¹ B) - tr( Λ⁻¹ Kuf' ΣQR^(-1/2) ΣQR^(-1/2) Kuf Λ⁻¹ B )\n                                                    ╰────────────────╯\n                                                        ≡ L\n              = tr(Λ⁻¹ B) - tr(L'*L*B)\n              = tr(Λ⁻¹ B) - dot(L, L*B)\n\nSee also: [``](@ref).\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.trinvAB-Tuple{PDMats.AbstractPDMat,Any}","page":"Sparse GPs","title":"GaussianProcesses.trinvAB","text":"trinvAB(A::AbstractPDMat, B)\n\nComputes tr(A⁻¹ B).\n\n\n\n\n\n","category":"method"},{"location":"sparse/#LinearAlgebra.logdet-Tuple{GaussianProcesses.FullyIndepPDMat}","page":"Sparse GPs","title":"LinearAlgebra.logdet","text":"The matrix determinant lemma states that\n    logdet(A+UWV') = logdet(W⁻¹ + V'A⁻¹U) + logdet(W) + logdet(A)\nSo for\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + Λ\n    logdet(Σ) = logdet(Kuu + Kuf Λ⁻¹ Kuf')       + logdet(Kuu⁻¹) + logdet(Λ)\n              = logdet(        ΣQR             ) - logdet(Kuu)   + logdet(Λ)\n\n\n\n\n\n","category":"method"},{"location":"sparse/#LinearAlgebra.tr-Tuple{GaussianProcesses.FullyIndepPDMat}","page":"Sparse GPs","title":"LinearAlgebra.tr","text":"tr(a::FullyIndepPDMat)\n\nTrace of the FITC approximation to the covariance matrix:\n\ntr(Σ) = tr(Kuf' Kuu⁻¹ Kuf + Λ)\n      = tr(Kuf' Kuu⁻¹ Kuf) + tr(Λ)\n      = tr(Kuf' Kuu^{-1/2} Kuu^{-1/2} Kuf) + tr(Λ)\n                          ╰──────────────╯\n                             ≡  Lk\n      = dot(Lk, Lk) + sum(diag(Λ))\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.SubsetOfRegsPDMat","page":"Sparse GPs","title":"GaussianProcesses.SubsetOfRegsPDMat","text":"Subset of Regressors sparse positive definite matrix.\n\n\n\n\n\n","category":"type"},{"location":"sparse/#Base.:\\-Tuple{GaussianProcesses.SubsetOfRegsPDMat,Any}","page":"Sparse GPs","title":"Base.:\\","text":"We have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + σ²I\nBy Woodbury\n    Σ⁻¹ = σ⁻²I - σ⁻⁴ Kuf'(Kuu + σ⁻² Kuf Kuf')⁻¹ Kuf\n        = σ⁻²I - σ⁻⁴ Kuf'(       ΣQR        )⁻¹ Kuf\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_kern!-Tuple{AbstractArray{T,1} where T,Kernel,AbstractArray{T,2} where T,PDMats.AbstractPDMat,GaussianProcesses.SparseKernelData,AbstractArray{T,1} where T,Any,Any,Any,Any,Any,Any,Any,GaussianProcesses.SubsetOfRegsStrategy}","page":"Sparse GPs","title":"GaussianProcesses.dmll_kern!","text":"dmll_kern!(dmll::AbstractVector, kernel::Kernel, X::AbstractMatrix, cK::SubsetOfRegsPDMat, kerneldata::KernelData, ααinvcKI::AbstractMatrix, covstrat::SubsetOfRegsStrategy)\n\nDerivative of the log likelihood under the Subset of Regressors (SoR) approximation.\n\nHelpful reference: Vanhatalo, Jarno, and Aki Vehtari.                    \"Sparse log Gaussian processes via MCMC for spatial epidemiology.\"                    In Gaussian processes in practice, pp. 73-89. 2007.\n\nGenerally, for a multivariate normal with zero mean     ∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)                     ╰───────────────╯     ╰──────────╯                            V                 T\n\nwhere Σ = Kff + σ²I.\n\nNotation: f is the observations, u is the inducing points.           ∂X stands for ∂X/∂θ, where θ is the kernel hyperparameters.\n\nIn the SoR approximation, we replace Kff with Qff = Kfu Kuu⁻¹ Kuf\n\n∂Σ = ∂(Qff) = ∂(Kfu Kuu⁻¹ Kuf)             = ∂(Kfu) Kuu⁻¹ Kuf + Kfu ∂(Kuu⁻¹) Kuf + Kfu Kuu⁻¹ ∂(Kuf)\n\n∂(Kuu⁻¹) = -Kuu⁻¹ ∂(Kuu) Kuu⁻¹  ––––^\n\nAlso have pre-computed α = Σ⁻¹ y, so V can now be computed efficiency (O(nm²) I think…) by careful ordering of the matrix multiplication steps.\n\nFor T, we use the identity tr(AB) = dot(A',B): tr(Σ⁻¹ ∂Σ) = 2 tr(Σ⁻¹ ∂(Kfu) Kuu⁻¹ Kuf) + tr(Σ⁻¹ Kfu ∂(Kuu⁻¹) Kuf)            = 2 dot((Σ⁻¹ ∂(Kfu))', Kuu⁻¹ Kuf) - tr(Σ⁻¹ Kfu Kuu⁻¹ ∂Kuu Kuu⁻¹ Kuf)            = 2 dot((Σ⁻¹ ∂(Kfu))', Kuu⁻¹ Kuf) - dot((Σ⁻¹ Kfu)', Kuu⁻¹ ∂Kuu Kuu⁻¹ Kuf) which again is computed in O(nm²).\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.dmll_noise-Tuple{GPE,GaussianProcesses.SoRPrecompute,GaussianProcesses.SubsetOfRegsStrategy}","page":"Sparse GPs","title":"GaussianProcesses.dmll_noise","text":"dmll_noise(gp::GPE, precomp::SoRPrecompute)\n\n∂logp(Y|θ) = 1/2 y' Σ⁻¹ ∂Σ Σ⁻¹ y - 1/2 tr(Σ⁻¹ ∂Σ)\n\n∂Σ = I for derivative wrt σ², so ∂logp(Y|θ) = 1/2 y' Σ⁻¹ Σ⁻¹ y - 1/2 tr(Σ⁻¹)             = 1/2[ dot(α,α) - tr(Σ⁻¹) ]\n\nΣ⁻¹ = σ⁻²I - σ⁻⁴ Kuf'(Kuu + σ⁻² Kuf Kuf')⁻¹ Kuf     = σ⁻²I - σ⁻⁴ Kuf'(       ΣQR        )⁻¹ Kuf\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.get_alpha_u-Tuple{GaussianProcesses.SubsetOfRegsPDMat,AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.Mean}","page":"Sparse GPs","title":"GaussianProcesses.get_alpha_u","text":"alpha_u(Ktrain::SubsetOfRegsPDMat, xtrain::AbstractMatrix, ytrain::AbstractVector, m::Mean)\n\nΣQR⁻¹ Kuf Λ⁻¹ (y-μ)\n\n\n\n\n\n","category":"method"},{"location":"sparse/#GaussianProcesses.predictMVN-Tuple{AbstractArray{T,2} where T,AbstractArray{T,2} where T,AbstractArray{T,1} where T,Kernel,GaussianProcesses.Mean,AbstractArray{T,1} where T,GaussianProcesses.SubsetOfRegsStrategy,PDMats.AbstractPDMat}","page":"Sparse GPs","title":"GaussianProcesses.predictMVN","text":"See Quiñonero-Candela and Rasmussen 2005, equations 16b.\nSome derivations can be found below that are not spelled out in the paper.\n\nNotation: Qab = Kau Kuu⁻¹ Kub\n          ΣQR = Kuu + σ⁻² Kuf Kuf'\n\n          x: prediction (test) locations\n          f: training (observed) locations\n          u: inducing point locations\n\nWe have\n    Σ ≈ Kuf' Kuu⁻¹ Kuf + σ²I\nBy Woodbury\n    Σ⁻¹ = σ⁻²I - σ⁻⁴ Kuf'(Kuu + σ⁻² Kuf Kuf')⁻¹ Kuf\n        = σ⁻²I - σ⁻⁴ Kuf'(       ΣQR        )⁻¹ Kuf\n\nThe predictive mean can be derived (assuming zero mean function for simplicity)\nμ = Qxf (Qff + σ²I)⁻¹ y\n  = Kxu Kuu⁻¹ Kuf [σ⁻²I - σ⁻⁴ Kuf' ΣQR⁻¹ Kuf] y   # see Woodbury formula above.\n  = σ⁻² Kxu Kuu⁻¹ [ΣQR - σ⁻² Kuf Kfu] ΣQR⁻¹ Kuf y # factoring out common terms\n  = σ⁻² Kxu Kuu⁻¹ [Kuu] ΣQR⁻¹ Kuf y               # using definition of ΣQR\n  = σ⁻² Kxu ΣQR⁻¹ Kuf y                           # matches equation 16b\n\nSimilarly for the posterior predictive covariance:\nΣ = Qxx - Qxf (Qff + σ²I)⁻¹ Qxf'\n  = Qxx - σ⁻² Kxu ΣQR⁻¹ Kuf Qxf'                # substituting result from μ\n  = Qxx - σ⁻² Kxu ΣQR⁻¹  Kuf Kfu    Kuu⁻¹ Kux   # definition of Qxf\n  = Qxx -     Kxu ΣQR⁻¹ (ΣQR - Kuu) Kuu⁻¹ Kux   # using definition of ΣQR\n  = Qxx - Kxu Kuu⁻¹ Kux + Kxu ΣQR⁻¹ Kux         # expanding\n  = Qxx - Qxx           + Kxu ΣQR⁻¹ Kux         # definition of Qxx\n  = Kxu ΣQR⁻¹ Kux                               # simplifying\n\n\n\n\n\n","category":"method"},{"location":"kernels/#Kernels-1","page":"Kernels","title":"Kernels","text":"","category":"section"},{"location":"kernels/#","page":"Kernels","title":"Kernels","text":"Modules = [GaussianProcesses]\nPages = readdir(joinpath(\"..\", \"src\", \"kernels\"))","category":"page"},{"location":"kernels/#GaussianProcesses.Const","page":"Kernels","title":"GaussianProcesses.Const","text":"Const <: Kernel\n\nConstant kernel\n\nk(xx) = σ²\n\nwith signal standard deviation σ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Const-Union{Tuple{T}, Tuple{T}} where T","page":"Kernels","title":"GaussianProcesses.Const","text":"Constant kernel function\n\nConst(lσ::T)\n\nArguments\n\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Lin-Tuple{Real}","page":"Kernels","title":"GaussianProcesses.Lin","text":"Lin(ll::Union{Real,Vector{Real}})\n\nCreate linear kernel with length scale exp.(ll).\n\nSee also LinIso and LinArd.\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.LinArd","page":"Kernels","title":"GaussianProcesses.LinArd","text":"LinArd <: Kernel\n\nARD linear kernel (covariance)\n\nk(xx) = xᵀL²x\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ) and L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.LinArd-Union{Tuple{Array{T,1}}, Tuple{T}} where T","page":"Kernels","title":"GaussianProcesses.LinArd","text":"Linear ARD Covariance Function\n\nLinArd(ll::Vector{T})\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.LinIso","page":"Kernels","title":"GaussianProcesses.LinIso","text":"LinIso <: Kernel\n\nIsotropic linear kernel (covariance)\n\nk(x x) = xᵀxℓ²\n\nwith length scale ℓ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.LinIso-Union{Tuple{T}, Tuple{T}} where T","page":"Kernels","title":"GaussianProcesses.LinIso","text":"Linear Isotropic Covariance Function\n\nLinIso(ll::T)\n\nArguments\n\nll::Real: length scale (given on log scale)\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Masked","page":"Kernels","title":"GaussianProcesses.Masked","text":"Masked{K<:Kernel} <: Kernel\n\nA wrapper for kernels so that they are only applied along certain dimensions.\n\nThis is similar to the active_dims kernel attribute in the python GPy package and to the covMask function in the matlab gpml package.\n\nThe implementation is very simple: any function of the kernel that takes an X::Matrix input is delegated to the wrapped kernel along with a view of X that only includes the active dimensions.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Matern-Tuple{Real,Real,Real}","page":"Kernels","title":"GaussianProcesses.Matern","text":"Matern(ν::Real, ll::Union{Real,Vector{Real}}, lσ::Real)\n\nCreate Matérn kernel of type ν (i.e. ν = 1/2, ν = 3/2, or ν = 5/2) with length scale exp.(ll) and signal standard deviation exp(σ).\n\nSee also Mat12Iso, Mat12Ard, Mat32Iso, Mat32Ard, Mat52Iso, and Mat52Ard.\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat12Ard","page":"Kernels","title":"GaussianProcesses.Mat12Ard","text":"Mat12Ard <: MaternARD\n\nARD Matern 1/2 kernel (covariance)\n\nk(xx) = σ² exp(-x-xL)\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ) and signal standard deviation σ where L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat12Ard-Union{Tuple{T}, Tuple{Array{T,1},T}} where T","page":"Kernels","title":"GaussianProcesses.Mat12Ard","text":"Matern 1/2 ARD covariance Function\n\nMat12Ard(ll::Vector{T}, lσ::T)\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat12Iso","page":"Kernels","title":"GaussianProcesses.Mat12Iso","text":"Mat12Iso <: MaternISO\n\nIsotropic Matern 1/2 kernel (covariance)\n\nk(xx) = σ^2 exp(-x-yℓ)\n\nwith length scale ℓ and signal standard deviation σ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat12Iso-Union{Tuple{T}, Tuple{T,T}} where T","page":"Kernels","title":"GaussianProcesses.Mat12Iso","text":"Matern 1/2 isotropic covariance Function\n\nMat12Iso(ll::T, lσ::T)\n\nArguments\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat32Ard","page":"Kernels","title":"GaussianProcesses.Mat32Ard","text":"Mat32Ard <: MaternARD\n\nARD Matern 3/2 kernel (covariance)\n\nk(xx) = σ²(1 + 3x-xL)exp(- 3x-xL)\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ) and signal standard deviation σ where L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat32Ard-Union{Tuple{T}, Tuple{Array{T,1},T}} where T","page":"Kernels","title":"GaussianProcesses.Mat32Ard","text":"Matern 3/2 ARD covariance function\n\nMat32Ard(ll::Vector{T}, lσ::T)\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat32Iso","page":"Kernels","title":"GaussianProcesses.Mat32Iso","text":"Mat32Iso <: MaternIso\n\nIsotropic Matern 3/2 kernel (covariance)\n\nk(xx) = σ²(1 + 3x-xℓ)exp(-3x-xℓ)\n\nwith length scale ℓ and signal standard deviation σ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat32Iso-Union{Tuple{T}, Tuple{T,T}} where T","page":"Kernels","title":"GaussianProcesses.Mat32Iso","text":"Matern 3/2 isotropic covariance function\n\nMat32Iso(ll::T, lσ::T)\n\nArguments\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat52Ard","page":"Kernels","title":"GaussianProcesses.Mat52Ard","text":"Mat52Ard <: MaternARD\n\nARD Matern 5/2 kernel (covariance)\n\nk(xx) = σ²(1 + 5x-xL + 5x-x²(3L²))exp(- 5x-xL)\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ) and signal standard deviation σ where L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat52Ard-Union{Tuple{T}, Tuple{Array{T,1},T}} where T","page":"Kernels","title":"GaussianProcesses.Mat52Ard","text":"Matern 5/2 ARD covariance Function\n\nMat52Ard(ll::Vector{Real}, lσ::Real)\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Mat52Iso","page":"Kernels","title":"GaussianProcesses.Mat52Iso","text":"Mat52Iso <: MaternIso\n\nIsotropic Matern 5/2 kernel (covariance)\n\nk(xx) = σ²(1+5x-xℓ + 5x-x²(3ℓ²))exp(- 5x-xℓ)\n\nwith length scale ℓ and signal standard deviation σ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Mat52Iso-Union{Tuple{T}, Tuple{T,T}} where T","page":"Kernels","title":"GaussianProcesses.Mat52Iso","text":"Matern 5/2 isotropic covariance function\n\nMat52Iso(ll::Real, lσ::Real)\n\nArguments\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Noise","page":"Kernels","title":"GaussianProcesses.Noise","text":"Noise <: Kernel\n\nNoise kernel (covariance)\n\nk(xx) = σ²δ(x-x)\n\nwhere δ is the Kronecker delta function and σ is the signal standard deviation.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Noise-Union{Tuple{T}, Tuple{T}} where T","page":"Kernels","title":"GaussianProcesses.Noise","text":"White Noise kernel\n\nNoise(lσ::Real)\n\nArguments\n\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Periodic","page":"Kernels","title":"GaussianProcesses.Periodic","text":"Periodic <: Isotropic{Euclidean}\n\nPeriodic kernel (covariance)\n\nk(xx) = σ²exp(-2sin²(πx-xp)ℓ²)\n\nwith length scale ℓ, signal standard deviation σ, and period p.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Periodic-Union{Tuple{T}, Tuple{T,T,T}} where T","page":"Kernels","title":"GaussianProcesses.Periodic","text":"Periodic kernel function\n\nPeriodic(ll::Real, lσ::Real, lp::Real)\n\nArguments\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)\nlp::Real: periodicity parameter (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.Poly","page":"Kernels","title":"GaussianProcesses.Poly","text":"Poly <: Kernel\n\nPolynomial kernel (covariance)\n\nk(xx) = σ²(xᵀx + c)ᵈ\n\nwith signal standard deviation σ, additive constant c, and degree d.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.Poly-Union{Tuple{T}, Tuple{T,T,Int64}} where T","page":"Kernels","title":"GaussianProcesses.Poly","text":"Polynomial kernel function\n\nPoly(lc::Real, lσ::Real, deg::Int)\n\nArguments\n\nlc::Real: additive constant (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)\ndeg::Int: degree of polynomial  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.RQ-Tuple{Real,Real,Real}","page":"Kernels","title":"GaussianProcesses.RQ","text":"RQ(ll::Union{Real,Vector{Real}}, lσ::Real, lα::Real)\n\nCreate Rational Quadratic kernel with length scale exp.(ll), signal standard deviation exp(lσ), and shape parameter exp(lα).\n\nSee also RQIso and RQArd.\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.RQArd","page":"Kernels","title":"GaussianProcesses.RQArd","text":"RQArd <: StationaryARD{WeightedSqEuclidean}\n\nARD Rational Quadratic kernel (covariance)\n\nk(xx) = σ²(1 + (x - x)ᵀL²(x - x)(2α))^-α\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ), signal standard deviation σ, and shape parameter α where L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.RQArd-Union{Tuple{T}, Tuple{Array{T,1},T,T}} where T","page":"Kernels","title":"GaussianProcesses.RQArd","text":"Rational Quadratic ARD Covariance Function\n\nRQArd(ll::Vector{Real}, lσ::Real, lα::Real)\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)\nlα::Real: shape parameter (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.RQIso","page":"Kernels","title":"GaussianProcesses.RQIso","text":"RQIso <: Isotropic{SqEuclidean}\n\nIsotropic Rational Quadratic kernel (covariance)\n\nk(xx) = σ²(1 + (x - x)ᵀ(x - x)(2αℓ²))^-α\n\nwith length scale ℓ, signal standard deviation σ, and shape parameter α.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.RQIso-Union{Tuple{T}, Tuple{T,T,T}} where T","page":"Kernels","title":"GaussianProcesses.RQIso","text":"Rational Quadratic Isotropic Covariance Function\n\nRQIso(ll:T, lσ::T, lα::T)\n\nArguments\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)\nlα::Real: shape parameter (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.SE-Tuple{Real,Real}","page":"Kernels","title":"GaussianProcesses.SE","text":"SE(ll::Union{Real,Vector{Real}}, lσ::Real)\n\nCreate squared exponential kernel with length scale exp.(ll) and signal standard deviation exp(lσ).\n\nSee also SEIso and SEArd.\n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.SEArd","page":"Kernels","title":"GaussianProcesses.SEArd","text":"SEArd <: StationaryARD{WeightedSqEuclidean}\n\nARD Squared Exponential kernel (covariance)\n\nk(xx) = σ²exp(- (x - x)ᵀL²(x - x)2)\n\nwith length scale ℓ = (ℓ₁ ℓ₂ ) and signal standard deviation σ where L = diag(ℓ₁ ℓ₂ ).\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.SEArd-Union{Tuple{T}, Tuple{Array{T,1},T}} where T","page":"Kernels","title":"GaussianProcesses.SEArd","text":"Squared Exponential Function with ARD\n\nSEArd(ll::Vector{Real}, lσ::Real)\n\nArguments\n\nll::Vector{Real}: vector of length scales (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.SEIso","page":"Kernels","title":"GaussianProcesses.SEIso","text":"SEIso <: Isotropic{SqEuclidean}\n\nIsotropic Squared Exponential kernel (covariance)\n\nk(xx) = σ²exp(- (x - x)ᵀ(x - x)(2ℓ²))\n\nwith length scale ℓ and signal standard deviation σ.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.SEIso-Union{Tuple{T}, Tuple{T,T}} where T","page":"Kernels","title":"GaussianProcesses.SEIso","text":"Squared Exponential kernel function\n\nSEIso(ll::T, lσ::T)\n\nArguments:\n\nll::Real: length scale (given on log scale)\nlσ::Real: signal standard deviation (given on log scale)  \n\n\n\n\n\n","category":"method"},{"location":"kernels/#GaussianProcesses.EmptyData","page":"Kernels","title":"GaussianProcesses.EmptyData","text":"EmptyData <: KernelData\n\nDefault empty KernelData.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.KernelData","page":"Kernels","title":"GaussianProcesses.KernelData","text":"KernelData\n\nData to be used with a kernel object to calculate a covariance matrix, which is independent of kernel hyperparameters.\n\nSee also EmptyData.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#GaussianProcesses.cov!","page":"Kernels","title":"GaussianProcesses.cov!","text":"cov!(cK::AbstractMatrix, k::Kernel, X1::AbstractMatrix, X2::AbstractMatrix)\n\nLike cov(k, X1, X2), but stores the result in cK rather than a new matrix.\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Statistics.cov","page":"Kernels","title":"Statistics.cov","text":"cov(k::Kernel, X::AbstractMatrix[, data::KernelData = EmptyData()])\n\nCreate covariance matrix from kernel k, matrix of observations X, where each column is an observation, and kernel data data constructed from input observations.\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Statistics.cov","page":"Kernels","title":"Statistics.cov","text":"cov(k::Kernel, X1::AbstractMatrix, X2::AbstractMatrix)\n\nCreate covariance matrix from kernel k and matrices of observations X1 and X2, where each column is an observation.\n\n\n\n\n\n","category":"function"},{"location":"sparse_example/#Sparse-GPs-1","page":"Sparse GPs","title":"Sparse GPs","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"This notebook demonstrates the sparse GP approximations implemented in the GaussianProcesses.jl. We will simulate a dataset, fit if first using the exact representation, then fit it using sparse GPs. For each approximation, this will show the tradeoffs involved, and the performance gains.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"# imports and set up plots\nusing Distributions\nusing LinearAlgebra: diag\nusing Random\nusing GaussianProcesses\nimport Statistics\n\nimport PyPlot; plt=PyPlot\nusing LaTeXStrings\ncbbPalette = [\"#E69F00\", \"#56B4E9\", \"#009E73\",\n                \"#F0E442\", \"#0072B2\", \"#D55E00\",\n                \"#CC79A7\"];","category":"page"},{"location":"sparse_example/#Simulated-data-1","page":"Sparse GPs","title":"Simulated data","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"We start by simulating some arbitrary data, with large noise and a fairly large sample size (n=5,000) so the effects of the approximations are clear.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"\"\"\" The true function we will be simulating from. \"\"\"\nfunction fstar(x::Float64)\n    return abs(x-5)*cos(2*x)\nend\n\nσy = 10.0\nn=5000\n\nRandom.seed!(1) # for reproducibility\nXdistr = Beta(7,7)\nϵdistr = Normal(0,σy)\nx = rand(Xdistr, n)*10\nX = Matrix(x')\nY = fstar.(x) .+ rand(ϵdistr,n)\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"xx = range(0, stop=10, length=200)\nplt.plot(x, Y, \".\", color=cbbPalette[1], label=\"simulated data\", alpha=0.1)\nplt.plot(xx, fstar.(xx), color=cbbPalette[2], label=L\"f^\\star(X)\", linewidth=2)\nplt.xlabel(L\"X\")\nplt.ylabel(L\"Y\")\nplt.title(\"Simulated Data\")\nplt.legend(loc=\"lower center\", fontsize=\"small\")\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#Exact-GP-inference-1","page":"Sparse GPs","title":"Exact GP inference","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"k = SEIso(log(0.3), log(5.0))\nGPE(X, Y, MeanConst(mean(Y)), k, log(σy)) # warm-up\n# time the second run (so the time doesn't include compilation):\n@time gp_full = GPE(X, Y, MeanConst(mean(Y)), k, log(σy))","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  1.618708 seconds (40 allocations: 572.320 MiB, 14.35% gc time)\n\n\n\n\n\nGP Exact object:\n  Dim = 1\n  Number of observations = 5000\n  Mean function:\n    Type: MeanConst, Params: [0.56185]\n  Kernel:\n    Type: SEIso{Float64}, Params: [-1.20397, 1.60944]\n  Input observations =\n[4.92176 5.27531 … 3.48002 5.43604]\n  Output observations = [-19.283, -6.07098, 3.33402, 12.6241, -14.5596, 20.8922, -7.86136, -3.41118, -0.686436, 9.39745  …  0.160936, 10.2597, -6.34116, 0.669071, -3.28242, 4.95583, 0.739365, 2.82739, 12.3229, 11.3255]\n  Variance of observation noise = 100.00000000000004\n  Marginal Log-Likelihood = -18640.795","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"# extract predictions\npredict_f(gp_full, xx; full_cov=true) # warm-up\n@time μ_exact, Σ_exact = predict_f(gp_full, xx; full_cov=true)\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.161568 seconds (79 allocations: 15.880 MiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"xx = range(0, stop=10, length=200)\nplt.plot(x,Y, \".\", color=\"black\", markeredgecolor=\"None\", alpha=0.1, label=\"simulated data\")\nplt.plot(xx, fstar.(xx), color=cbbPalette[2], label=L\"f^\\star(X)\", linewidth=2)\nplt.plot(xx, μ_exact, color=cbbPalette[3], label=L\"$f(x) \\mid Y, \\hat\\theta$\")\nplt.fill_between(xx, μ_exact.-sqrt.(diag(Σ_exact)), μ_exact.+sqrt.(diag(Σ_exact)),\n                 color=cbbPalette[3], alpha=0.5)\nplt.xlabel(L\"X\")\nplt.ylabel(L\"Y\")\nplt.title(\"Exact GP\")\nplt.legend(loc=\"lower center\", fontsize=\"small\")\nplt.ylim(-7.5,7.5)\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"The sparse Gaussian Process approximations implemented rely on the concept of inducing points, which we denote X_u (an p times m matrix of inducing points, where the dimensionality p=1 in this example). See the following article for a helpful introduction, from which we also use the naming conventions for the approximations:","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"Quiñonero-Candela, Joaquin, and Carl Edward Rasmussen. \"A unifying view of sparse approximate Gaussian process regression.\" Journal of Machine Learning Research 6, no. Dec (2005): 1939-1959.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"The inducing points can be randomly chosen from the data, or laid out on a grid, or even optimized as part of fitting the hyperparameters [Note: this feature is not yet available in GaussianProcesses.jl]. For illustration in this notebook, we will use quantiles of the covariate x:","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"Xu = Matrix(quantile(x, [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.98])')","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"1×12 Array{Float64,2}:\n 3.82567  4.06404  4.25939  4.4478  …  5.33994  5.53788  5.73281  7.64571","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"We will reuse the same inducing points for all methods.","category":"page"},{"location":"sparse_example/#Sparse-approximations-1","page":"Sparse GPs","title":"Sparse approximations","text":"","category":"section"},{"location":"sparse_example/#Subset-of-Regressors-1","page":"Sparse GPs","title":"Subset of Regressors","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"We first demonstrate the subset of regressors method, the simplest and quickest approximation offered in the package. The shortcut function SoR(X, Xu, Y, m, k, logNoise) creates a Gaussian process object","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"gp_SOR = GaussianProcesses.SoR(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));\n@time gp_SOR = GaussianProcesses.SoR(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.002714 seconds (68 allocations: 2.032 MiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"In this example, we see a more than 500-fold speed-up, and similarly reduced memory footprint. Generating predictions (below) is also immensely faster. But is the approximation any good? We show the prediction alongside the exact solution for comparison.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"predict_f(gp_SOR, xx; full_cov=true)\n@time predict_f(gp_SOR, xx; full_cov=true);","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.000266 seconds (37 allocations: 492.828 KiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"function plot_approximation(gp_exact, gp_approx, approx_label)\n    μ_exact, Σ_exact = predict_f(gp_full, xx; full_cov=true)\n    μapprox, Σapprox = predict_f(gp_approx, xx; full_cov=true)\n    plt.plot(x,Y, \".\", color=\"black\", markeredgecolor=\"None\", alpha=0.1, label=\"simulated data\")\n    plt.plot(xx, fstar.(xx), color=cbbPalette[2], label=L\"f^\\star(X)\", linewidth=2)\n    plt.plot(xx, μ_exact, color=cbbPalette[3], label=L\"$f(x) \\mid Y, \\hat\\theta$\")\n    plt.fill_between(xx, μ_exact.-sqrt.(diag(Σ_exact)), μ_exact.+sqrt.(diag(Σ_exact)),\n                     color=cbbPalette[3], alpha=0.3)\n    plt.plot(xx, μapprox, color=cbbPalette[6], label=L\"$f(x) \\mid Y, \\hat\\theta$\"*approx_label)\n    y_err = sqrt.(diag(Σapprox))\n    plt.fill_between(xx, μapprox.-y_err, μapprox.+y_err,\n                     color=cbbPalette[6], alpha=0.5)\n    plt.xlabel(L\"X\")\n    plt.ylabel(L\"Y\")\n    plt.plot(vec(Xu), fill(0.0, length(Xu)).-5, linestyle=\"None\",\n        marker=6, markersize=12, color=\"black\", label=L\"inducing points $X_\\mathbf{u}$\")\n    plt.legend(loc=\"upper center\", fontsize=\"small\")\n    plt.ylim(-10, 10)\nend\nplot_approximation(gp_full, gp_SOR, \"SoR\")\nplt.title(\"Subset of Regressors\")\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"The locations of the inducing points are shown with black triangles (their y-coordinate is arbitrary). The approximation is seen as the difference between the exact (green) mean prediction and credible envelop, and the predictions from SoR (orange). We can readily see that the approximation is reasonably good near the inducing points, but degrades quickly away from them. Most worryingly, the extrapolation behaviour is dangerous: the posterior variance is vastly underestimated (it goes to zero), which would lead to very misleading inference results. This is in line with the academic literature, like the Q&R 2005 article cited above.","category":"page"},{"location":"sparse_example/#Deterministic-Training-Conditionals-1","page":"Sparse GPs","title":"Deterministic Training Conditionals","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"We move on to the next approximation, “Deterministic Training Conditionals”, which does not approximate the prior variance as zero away from inducing points, and therefore should lead to better inference.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"gp_DTC = GaussianProcesses.DTC(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));\n@time gp_DTC = GaussianProcesses.DTC(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.003959 seconds (70 allocations: 2.033 MiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"μDTCgp, ΣDTCgp = predict_f(gp_DTC, xx; full_cov=true)\n@time predict_f(gp_DTC, xx; full_cov=true);","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.002262 seconds (61 allocations: 2.064 MiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"plot_approximation(gp_full, gp_DTC, \"DTC\")\nplt.title(\"Deterministic Training Conditionals\")\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"It is just as fast as SoR, but has conservative predictive variance away from the inducing points, which reflects the information removed by the sparse approximation, and which is safer for inference. The mean prediction is in fact mathematically the same as in SoR, so there is no improvement there. For these reasons, in general DTC should be preferred over SoR.","category":"page"},{"location":"sparse_example/#Fully-Independent-Training-Conditionals-1","page":"Sparse GPs","title":"Fully Independent Training Conditionals","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"The Fully Independent Training Conditionals (FITC) sparse approximation goes one step further, and adds a diagonal correction to the sparse covariance approximation of SoR and DTC (see Q&R 2005 for details).","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"gp_FITC = GaussianProcesses.FITC(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));\n@time gp_FITC = GaussianProcesses.FITC(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.017552 seconds (10.09 k allocations: 3.902 MiB, 60.36% gc time)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"predict_f(gp_FITC, xx; full_cov=true)\n@time predict_f(gp_FITC, xx; full_cov=true);","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.002084 seconds (63 allocations: 2.064 MiB)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"plot_approximation(gp_full, gp_FITC, \"FITC\")\nplt.title(\"Fully Independent Training Conditionals\")\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"As anticipated in Q&R 2005, the improvement over DTC is actually fairly minimal. However, the computational time is significantly higher (though still much lower than the exact GP). Consequently, for most applications, DTC may be preferable to FITC.","category":"page"},{"location":"sparse_example/#Full-Scale-Approximation-1","page":"Sparse GPs","title":"Full Scale Approximation","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"The Full Scale Approximation (FSA) using the sparse approximation for long-range covariances, but is exact within smaller local blocks. The blocks need to be chosen in addition to the inducing points. In this notebook, we will create as many blocks as there are inducing points, and assign each observation to the block with the nearest inducing points.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"inearest = [argmin(abs.(xi.-Xu[1,:])) for xi in x]\nblockindices = [findall(isequal(i), inearest) for i in 1:size(Xu,2)]\n\nGaussianProcesses.FSA(X, Xu, blockindices, Y, MeanConst(mean(Y)), k, log(σy));\n@time gp_FSA = GaussianProcesses.FSA(X, Xu, blockindices, Y, MeanConst(mean(Y)), k, log(σy));","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.260956 seconds (593 allocations: 156.025 MiB, 43.57% gc time)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"For predictions, we also need to provide the block indices.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"iprednearest = [argmin(abs.(xi.-Xu[1,:])) for xi in xx]\nblockindpred = [findall(isequal(i), iprednearest)\n                for i in 1:size(Xu,2)]\n\npredict_f(gp_FSA, xx, blockindpred; full_cov=true)\n@time predict_f(gp_FSA, xx, blockindpred; full_cov=true);","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"  0.126982 seconds (616 allocations: 92.226 MiB, 6.72% gc time)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"function plot_approximation(gp_exact, gp_approx, approx_label, blockindpred)\n    μ_exact, Σ_exact = predict_f(gp_full, xx; full_cov=true)\n    μapprox, Σapprox = predict_f(gp_approx, xx, blockindpred; full_cov=true)\n    plt.plot(x,Y, \".\", color=\"black\", markeredgecolor=\"None\", alpha=0.1, label=\"simulated data\")\n    plt.plot(xx, fstar.(xx), color=cbbPalette[2], label=L\"f^\\star(X)\", linewidth=2)\n    plt.plot(xx, μ_exact, color=cbbPalette[3], label=L\"$f(x) \\mid Y, \\hat\\theta$\")\n    plt.fill_between(xx, μ_exact.-sqrt.(diag(Σ_exact)), μ_exact.+sqrt.(diag(Σ_exact)),\n                     color=cbbPalette[3], alpha=0.3)\n    plt.plot(xx, μapprox, color=cbbPalette[6], label=L\"$f(x) \\mid Y, \\hat\\theta$\"*approx_label)\n    y_err = sqrt.(diag(Σapprox))\n    plt.fill_between(xx, μapprox.-y_err, μapprox.+y_err,\n                     color=cbbPalette[6], alpha=0.5)\n    plt.xlabel(L\"X\")\n    plt.ylabel(L\"Y\")\n    plt.plot(vec(Xu), fill(0.0, length(Xu)).-5, linestyle=\"None\",\n        marker=6, markersize=12, color=\"black\", label=L\"inducing points $X_\\mathbf{u}$\")\n    plt.legend(loc=\"top left\", fontsize=\"small\")\n    plt.ylim(-10, 10)\nend\n# from StatsBase.jl:\nmidpoints(v::AbstractVector) = [Statistics.middle(v[i - 1], v[i]) for i in 2:length(v)]\nplt.axvline.(midpoints(vec(Xu)), alpha=0.9, color=cbbPalette[7], zorder=-1)\nplot_approximation(gp_full, gp_FSA, \"FSA\", blockindpred)\n;","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"(Image: png)","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"We also show the dividing lines between blocks. As you can see, the approximation is much improved compared to the previous approaches. It is only at the dividing lines between blocks (shown as pink vertical lines) that information does not travel, and so we can see jumps in the predictions if the block division is also far away from an inducing points. The downside is that we pay the computational cost of the full inference within blocks, so the speed-up compared to the full analytic solution is less impressive.","category":"page"},{"location":"sparse_example/#Under-the-hood-1","page":"Sparse GPs","title":"Under the hood","text":"","category":"section"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"When we use the shortcut functions shown in this notebook — SoR, DTC, FITC and FSA — a Gaussian process object is created with a special CovarianceStrategy structure, which stores the sparse approximation choice and parameters. Julia's multiple dispatch mechanism is then used in relevant GP methods to adapt the GP functionality according to which covariance strategy is used. It is therefore relatively straightforward for users to implement new approximations by creating a custom CovarianceStrategy structure, and implementing the methods that are affected by the approximation.","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"gp_SOR = GaussianProcesses.SoR(X, Xu, Y, MeanConst(mean(Y)), k, log(σy));\ngp_SOR.covstrat","category":"page"},{"location":"sparse_example/#","page":"Sparse GPs","title":"Sparse GPs","text":"GaussianProcesses.SubsetOfRegsStrategy{Array{Float64,2}}([3.82567 4.06404 … 5.73281 7.64571])","category":"page"},{"location":"Regression/#Simple-GP-Regression-1","page":"Simple GP Regression","title":"Simple GP Regression","text":"","category":"section"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Gaussian processes are a powerful tool for nonlinear regression models. ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Assume that we have predictor variables mathbfX = mathbfx_i_i=1^N in mathbbR^d and response variables mathbfy=y_i in mathbbR_i=1^N.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The response variables mathbfy are assumed to dependent on the predictors mathbfX,","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"y_i sim mathcalN(f(mathbfx_i)sigma^2)  i=1ldotsn  ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"where f is a mapping function. Treating f as a random function, we assume that the distribution over f is a Gaussian process,","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"f sim mathcalGP(m(mathbfx)k(mathbfxmathbfx))","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"where m(cdot) and k(cdotcdot) are the mean and kernel functions respectively.","category":"page"},{"location":"Regression/#D-regression-example-1","page":"Simple GP Regression","title":"1D regression example","text":"","category":"section"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"We start by simulating some data","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"using GaussianProcesses\nusing Random\n\nRandom.seed!(20140430)\n# Training data\nn=10;                          #number of training points\nx = 2π * rand(n);              #predictors\ny = sin.(x) + 0.05*randn(n);   #regressors","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The first step in modelling with Gaussian Processes is to choose mean functions and kernels which describe the process. ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Note that all hyperparameters for the mean and kernel functions and sigma are given on the log scale. This is true for all strictly positive hyperparameters. Gaussian Processes are represented by objects of type 'GP' and constructed from observation data, a mean function and kernel, and optionally the amount of observation noise.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"#Select mean and covariance function\nmZero = MeanZero()                   #Zero mean function\nkern = SE(0.0,0.0)                   #Sqaured exponential kernel (note that hyperparameters are on the log scale)\n\nlogObsNoise = -1.0                        # log standard deviation of observation noise (this is optional)\ngp = GP(x,y,mZero,kern,logObsNoise)       #Fit the GP","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"GP Exact object:\n  Dim = 1\n  Number of observations = 10\n  Mean function:\n    Type: MeanZero, Params: Float64[]\n  Kernel:\n    Type: SEIso{Float64}, Params: [0.0, 0.0]\n  Input observations = \n[4.85461 5.17653 … 1.99412 3.45676]\n  Output observations = [-0.967293, -1.00705, -1.0904, 0.881121, -0.333213, -0.976965, 0.915934, 0.736218, 0.950849, -0.306432]\n  Variance of observation noise = 0.1353352832366127\n  Marginal Log-Likelihood = -6.335","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Once we've fit the GP function to the data, we can calculate the predicted mean and variance of the function at unobserved points mathbfx^asty^ast, conditional on the observed data mathcalD=mathbfymathbfX. This is done with the predict_y function.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The predict_y function returns the mean vector mu(mathbfx^ast) and covariance matrix (variance vector if full_cov=false) Sigma(mathbfx^astmathbfx^ast^top) of the predictive distribution,","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"    y^astmathbfx^astmathcalD sim mathcalN(mu(mathbfx^ast)Sigma(mathbfx^astmathbfx^ast^top)+sigma^2mathbfI)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"where","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"beginaligned\nmu(mathbfx^ast) = k(mathbfx^astmathbfX)(k(mathbfX mathbfX) + sigma_n^2 mathbfI)^-1mathbfy  \nSigma(mathbfx^astmathbfx^ast) = k(mathbfx^astmathbfx^ast) -k(mathbfx^astmathbfX)(k(mathbfX mathbfX)+ sigma_n^2 mathbfI)^-1 k(mathbfXmathbfx^ast)\nendaligned","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Note you can use the predict_f function to predict the latent function mathbff^ast.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"μ, σ² = predict_y(gp,range(0,stop=2π,length=100));","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"([0.357625, 0.384852, 0.412943, 0.441807, 0.471344, 0.501442, 0.53198, 0.562826, 0.593838, 0.624867  …  -0.669223, -0.63363, -0.597926, -0.562345, -0.527104, -0.492406, -0.458434, -0.425355, -0.393315, -0.362442], [0.603651, 0.557693, 0.512299, 0.468128, 0.425831, 0.386031, 0.349295, 0.316113, 0.286872, 0.261843  …  0.434056, 0.473396, 0.514594, 0.557168, 0.600593, 0.644326, 0.68782, 0.730548, 0.772021, 0.811799])","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Plotting GPs is straightforward and utilises the recipes approach to plotting from the Plots.jl package. More information about plotting GPs and the available functionality can be found in this Plotting with GaussianProcesses.jl.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The default plot function plot(gp) outputs the predicted mean and variance of the function (i.e. uses predict_f in the background), with the uncertainty in the function represented by a confidence ribbon (set to 95% by default). All optional plotting arguments are given after ;.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"using Plots  #Load Plots.jl package\n\nplot(gp; xlabel=\"x\", ylabel=\"y\", title=\"Gaussian process\", legend=false, fmt=:png)      # Plot the GP","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"(Image: png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The hyperparameters are optimized using the Optim.jl package. This offers users a range of optimization algorithms which can be applied to estimate the hyperparameters using type II maximum likelihood estimation. Gradients are available for all mean and kernel functions used in the package and therefore it is recommended that the user utilizes gradient based optimization techniques. As a default, the optimize! function uses the L-BFGS solver, however, alternative solvers can be applied. ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"using Optim\noptimize!(gp; method=ConjugateGradient())   # Optimise the hyperparameters","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Results of Optimization Algorithm\n * Algorithm: Conjugate Gradient\n * Starting Point: [-1.0,0.0,0.0]\n * Minimizer: [-2.992856448832551,0.4636861230870647, ...]\n * Minimum: -3.275745e+00\n * Iterations: 27\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false \n     |x - x'| = 4.62e-09 \n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -4.12e-14 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false \n     |g(x)| = 5.69e-08 \n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 66\n * Gradient Calls: 41","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"plot(gp; legend=false, fmt=:png)   #Plot the GP after the hyperparameters have been optimised ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"(Image: png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"By default all hyperparameters or optimized. But the function optimize! allows also to force hyperparameters to remain constant or optimize them in a box.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"optimize!(gp; kern = false)   # Don't optimize kernel hyperparameters\noptimize!(gp; kernbounds = [[-1, -1], [1, 1]]) # Optimize the kernel parameters in a box with lower bounds [-1, -1] and upper bounds [1, 1]","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Results of Optimization Algorithm\n * Algorithm: Fminbox with L-BFGS\n * Starting Point: [-2.992856448832551,0.4636861230870647, ...]\n * Minimizer: [-2.992856448832551,0.4636861230870647, ...]\n * Minimum: -3.275745e+00\n * Iterations: 1\n * Convergence: true\n   * |x - x'| ≤ 0.0e+00: true \n     |x - x'| = 0.00e+00 \n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false \n     |g(x)| = 6.32e-08 \n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 3\n * Gradient Calls: 3","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"MCMC, specifically a Hamiltonian Monte Carlo in this instance, can be run on the GPE hyperparameters through the mcmc function. Priors for hyperparameters of the mean and kernel parameters can be set through the set_priors! function. The log noise parameter of the GPE is a Uniform(0,1) distribution and currently can't be changed.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"using Distributions\n\nset_priors!(kern, [Normal(), Normal()]) # Uniform(0,1) distribution assumed by default if priors not specified\nchain = mcmc(gp)\nplot(chain', label=[\"Noise\", \"SE log length\", \"SE log scale\"]; fmt=:png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Number of iterations = 1000, Thinning = 1, Burn-in = 1 \nStep size = 0.100000, Average number of leapfrog steps = 9.786000 \nNumber of function calls: 9787\nAcceptance rate: 0.959000","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"(Image: png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"There is additional support for inference to be done in a fully Bayesian fashion through the use of an elliptical slice sampler. While a HMC sampler can often be shown to be highly efficient, the sampler's efficiency can be highly dependent upon good initial choice of the sampler's hyperparameters. Conversely, ESS has no free parameters and is designed to be highly efficient in tightly correlated Gaussian posteriors: a geometry commonly found in Gaussian process models.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Note currently, inference via an ESS is only supported when the likelihood is Gaussian. ","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"For advice on how to fit Gaussian processes with non-Gaussian data, see our documentation on Poisson regression or classification.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"mhmc = mean(chain, dims=2)\nmess = mean(ess_chain, dims=2)\n","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"3×1 Array{Float64,2}:\n -2.444901016768212  \n  0.3721835201083439 \n -0.10584635335452222","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"for (a, b) in zip(mhmc, mess)\n    eq = abs(a-b)\n    idx = max(abs(a), abs(b))\n    return eq < idx/5\nend","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"true","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"mZero = MeanZero()                   #Zero mean function\nkern = SE(0.0,0.0)                   #Sqaured exponential kernel (note that hyperparameters are on the log scale)\nlogObsNoise = -1.0                        # log standard deviation of observation noise (this is optional)\n\ngpess = GP(x, y, mZero, kern, )       #Fit the GP\n\nset_priors!(kern, [Normal(), Normal()]) # Uniform(0,1) distribution assumed by default if priors not specified\nset_priors!(gpess.logNoise, [Distributions.Normal(-1.0, 1.0)])\ness_chain = ess(gpess)\nplot(chain', label=[\"Noise\", \"SE log length\", \"SE log scale\"]; fmt=:png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Number of iterations = 1000, Thinning = 1, Burn-in = 1 \nNumber of function calls: 6115\nAcceptance rate: 0.195503","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"(Image: png)","category":"page"},{"location":"Regression/#Multi-dimensional-regression-1","page":"Simple GP Regression","title":"Multi-dimensional regression","text":"","category":"section"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The regression example above can be easily extended to higher dimensions. For the purpose of visualisation, and without loss of generality, we consider a 2 dimensional regression example.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"#Training data\nd, n = 2, 50;         #Dimension and number of observations\nx = 2π * rand(d, n);                               #Predictors\ny = vec(sin.(x[1,:]).*sin.(x[2,:])) + 0.05*rand(n);  #Responses","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"For problems of dimension>1 we can use isotropic (Iso) kernels or automatic relevance determination (ARD) kernels. For Iso kernels, the length scale parameter ell is the same for all dimensions. For ARD kernels, each dimension has different length scale parameter.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"The Iso and ARD kernels are implemented automatically by replacing the single length scale parameter with a vector of parameters. For example, below we use the Matern 5/2 ARD kernel, if we wanted to use the Iso alternative then we would set the kernel as kern=Mat(5/2,0.0,0.0).","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"In this example we use a composite kernel represented as the sum of a Matern 5/2 ARD kernel and a Squared Exponential isotropic kernel. This is easily implemented using the + symbol, or in the case of a product kernel, using the * symbol (i.e. kern = Mat(5/2,[0.0,0.0],0.0) * SE(0.0,0.0)).","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"mZero = MeanZero()                             # Zero mean function\nkern = Matern(5/2,[0.0,0.0],0.0) + SE(0.0,0.0)    # Sum kernel with Matern 5/2 ARD kernel \n                                               # with parameters [log(ℓ₁), log(ℓ₂)] = [0,0] and log(σ) = 0\n                                               # and Squared Exponential Iso kernel with\n                                               # parameters log(ℓ) = 0 and log(σ) = 0","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Type: SumKernel{Mat52Ard{Float64},SEIso{Float64}}\n  Type: Mat52Ard{Float64}, Params: [-0.0, -0.0, 0.0]  Type: SEIso{Float64}, Params: [0.0, 0.0]","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Fit the Gaussian process to the data using the prespecfied mean and covariance functions.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"gp = GP(x,y,mZero,kern,-2.0)          # Fit the GP","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"GP Exact object:\n  Dim = 2\n  Number of observations = 50\n  Mean function:\n    Type: MeanZero, Params: Float64[]\n  Kernel:\n    Type: SumKernel{Mat52Ard{Float64},SEIso{Float64}}\n      Type: Mat52Ard{Float64}, Params: [-0.0, -0.0, 0.0]      Type: SEIso{Float64}, Params: [0.0, 0.0]\n  Input observations = \n[3.05977 4.74752 … 2.82127 5.38224; 2.02102 4.27258 … 6.13114 1.56497]\n  Output observations = [0.08509, 0.924505, 0.275745, -0.448035, -0.784758, -0.316803, -0.823483, -0.886726, 0.0059149, 0.414951  …  -0.413905, -0.347505, 0.46108, -0.204102, -0.538689, 0.554203, -0.874479, -0.0506017, -0.0215167, -0.745944]\n  Variance of observation noise = 0.01831563888873418\n  Marginal Log-Likelihood = -29.547","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Using the Optim package we have the option to choose from a range of optimize functions including conjugate gradients. It is also possible to fix the hyperparameters in either the mean function, kernel function or observation noise, by settting them to false in optimize! (e.g. optimize!(...,domean=false)).","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"optimize!(gp)                         # Optimize the hyperparameters","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"Results of Optimization Algorithm\n * Algorithm: L-BFGS\n * Starting Point: [-2.0,-0.0,-0.0,0.0,0.0,0.0]\n * Minimizer: [-4.277211995773057,5.536664077544042, ...]\n * Minimum: -5.023830e+01\n * Iterations: 34\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false \n     |x - x'| = 4.74e-02 \n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -1.36e-14 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false \n     |g(x)| = 1.87e-05 \n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 115\n * Gradient Calls: 115","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"A range of plotting options are availbe through the Plots.jl package.","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"plot(contour(gp) ,heatmap(gp); fmt=:png)","category":"page"},{"location":"Regression/#","page":"Simple GP Regression","title":"Simple GP Regression","text":"(Image: png)","category":"page"},{"location":"lik/#Likelihoods-1","page":"Likelihoods","title":"Likelihoods","text":"","category":"section"},{"location":"lik/#","page":"Likelihoods","title":"Likelihoods","text":"Modules = [GaussianProcesses]\nPages = readdir(joinpath(\"..\", \"src\", \"likelihoods\"))","category":"page"},{"location":"lik/#GaussianProcesses.BernLik","page":"Likelihoods","title":"GaussianProcesses.BernLik","text":"BernLik <: Likelihood\n\nBernoulli likelihood\n\np(y = k  f) = θᵏ (1 - θ)^1-k\n\nfor k  01, where θ = Φ(f) and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.BinLik","page":"Likelihoods","title":"GaussianProcesses.BinLik","text":"BinLik <: Likelihood\n\nBinomial likelihood\n\np(y = k  f) = k(n(n-k)) θᵏ(1 - θ)^n-k\n\nfor number of successes k  0 1  n out of n Bernoulli trials, where θ = exp(f)(1 + exp(f)) and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.ExpLik","page":"Likelihoods","title":"GaussianProcesses.ExpLik","text":"ExpLik <: Likelihood\n\nExponential likelihood\n\np(y  f) = θexp(-θy)\n\nwhere θ = exp(-f) and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.GaussLik","page":"Likelihoods","title":"GaussianProcesses.GaussLik","text":"GaussLik <: Likelihood\n\nGaussian, a.k.a. Normal, likelihood\n\np(y  f σ) = 1  (2πσ²) exp(-(y - f)²(2σ²))\n\nwhere standard deviation σ is a non-fixed hyperparameter and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.PoisLik","page":"Likelihoods","title":"GaussianProcesses.PoisLik","text":"PoisLik <: Likelihood\n\nPoisson likelihood\n\np(yᵢ = k  fᵢ) = θᵏexp(-θ)k\n\nfor k  N₀, where θ = exp(f) and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.StuTLik","page":"Likelihoods","title":"GaussianProcesses.StuTLik","text":"StuTLik <: Likelihood\n\nStudent-t likelihood (a.k.a. non-standardized Student's t-distribution)\n\np(y  f σ) = Γ((ν + 1)2)Γ(ν2)(πν)σ(1 + 1ν((y - f)σ)²)^-(ν + 1)2\n\nwith degrees of freedom ν  N₀, where scale σ is a non-fixed hyperparameter and f is the latent Gaussian process.\n\n\n\n\n\n","category":"type"},{"location":"lik/#GaussianProcesses.predict_obs-Tuple{Likelihood,AbstractArray{T,1} where T,AbstractArray{T,1} where T}","page":"Likelihoods","title":"GaussianProcesses.predict_obs","text":"Computes the predictive mean and variance given a Gaussian distribution for f using quadrature\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#Cross-Validation-1","page":"Cross Validation","title":"Cross Validation","text":"","category":"section"},{"location":"crossvalidation/#","page":"Cross Validation","title":"Cross Validation","text":"Modules = [GaussianProcesses]\nPages = [\"crossvalidation.jl\"]","category":"page"},{"location":"crossvalidation/#GaussianProcesses.dlogpdθ_CVfold-Tuple{GPE,AbstractArray{#s270,1} where #s270<:AbstractArray{Int64,1}}","page":"Cross Validation","title":"GaussianProcesses.dlogpdθ_CVfold","text":"dlogpdθ_CVfold_kern!(∂logp∂θ::AbstractVector{<:Real}, gp::GPE, folds::Folds; \n                     noise::Bool, domean::Bool, kern::Bool)\n\nDerivative of leave-one-out CV criterion with respect to the noise, mean and kernel hyperparameters. See Rasmussen & Williams equations 5.13.\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.dlogpdθ_LOO-Tuple{GPE}","page":"Cross Validation","title":"GaussianProcesses.dlogpdθ_LOO","text":"dlogpdθ_LOO(gp::GPE; noise::Bool, domean::Bool, kern::Bool)\n\nDerivatives of the leave-one-out CV criterion.\n\nSee also: logp_LOO, dlogpdθ_LOO_kern!, dlogpdσ2_LOO\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.dlogpdθ_LOO_kern!-Tuple{AbstractArray{#s267,1} where #s267<:Real,PDMats.PDMat,Kernel,AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.KernelData,AbstractArray{T,1} where T}","page":"Cross Validation","title":"GaussianProcesses.dlogpdθ_LOO_kern!","text":"dlogpdθ_LOO_kern!(gp::GPE)\n\nGradient of leave-one-out CV criterion with respect to the kernel hyperparameters. See Rasmussen & Williams equations 5.13.\n\nSee also: logp_LOO, dlogpdσ2_LOO, dlogpdθ_LOO\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.dlogpdσ2_LOO-Tuple{PDMats.PDMat,AbstractArray{T,2} where T,AbstractArray{T,1} where T,GaussianProcesses.KernelData,AbstractArray{T,1} where T}","page":"Cross Validation","title":"GaussianProcesses.dlogpdσ2_LOO","text":"dlogpdσ2_LOO(invΣ::PDMat, x::AbstractMatrix, y::AbstractVector, data::KernelData, alpha::AbstractVector)\n\nDerivative of leave-one-out CV criterion with respect to the logNoise parameter.\n\nSee also: logp_LOO, dlogpdθ_LOO_kern!, dlogpdθ_LOO\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.gradient_fold-Tuple{Any,Any,Any,Any,AbstractArray{Int64,1}}","page":"Cross Validation","title":"GaussianProcesses.gradient_fold","text":"gradient_fold(invΣ, alpha, ZjΣinv, Zjα, V::AbstractVector{Int})\n\nGradient with respect to the kernel hyperparameters of the CV criterion component for one validation fold:\n\n    nabla_thetaleft(log p (Y_V mid Y_T theta) right)\n\nwhere Y_V is the validation set and Y_T is the training set (all other observations) for this fold.\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.logp_CVfold-Tuple{GPE,AbstractArray{#s270,1} where #s270<:AbstractArray{Int64,1}}","page":"Cross Validation","title":"GaussianProcesses.logp_CVfold","text":"logp_CVfold(gp::GPE, folds::Folds)\n\nCV criterion for arbitrary fold.  A fold is a set of indices of the validation set.\n\nSee also predict_CVfold, logp_LOO\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.logp_LOO-Tuple{GPE}","page":"Cross Validation","title":"GaussianProcesses.logp_LOO","text":"logp_LOO(gp::GPE)\n\nLeave-one-out log probability CV criterion. This is implemented by  summing the normal log-pdf of each observation  with predictive LOO mean and variance parameters obtained by the predict_LOO function.\n\nSee also: logp_CVfold, update_mll!\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.predict_CVfold-Tuple{GPE,AbstractArray{#s270,1} where #s270<:AbstractArray{Int64,1}}","page":"Cross Validation","title":"GaussianProcesses.predict_CVfold","text":"predict_CVfold(gp::GPE, folds::Folds)\n\nCross-validated predictions for arbitrary folds.  A fold is a set of indices of the validation set. Returns predictions of yV (validation set) given all other observations yT (training set), as a vector of means and covariances. Using the notation from Rasmussen & Williams, see e.g. equation 5.12:     σᵢ = 𝕍 (yᵢ | y₋ᵢ)^(1/2)     μᵢ = 𝔼 (yᵢ | y₋ᵢ)\n\nThe output is the same as fitting the GP on xT,yT,  and calling predict_f on x_V to obtain the LOO predictive mean and covariance, repeated for each fold V. With GPs, this can thankfully be done analytically with a bit of linear algebra, which is what this function implements.\n\nSee also: predict_LOO\n\n\n\n\n\n","category":"method"},{"location":"crossvalidation/#GaussianProcesses.predict_LOO-Tuple{GPE}","page":"Cross Validation","title":"GaussianProcesses.predict_LOO","text":"predict_LOO(gp::GPE)\n\nLeave-one-out cross-validated predictions.  Returns predictions of yᵢ given all other observations y₋ᵢ, as a vector of means and variances. Using the notation from Rasmussen & Williams, see e.g. equation 5.12:     σᵢ = 𝕍 (yᵢ | y₋ᵢ)^(1/2)     μᵢ = 𝔼 (yᵢ | y₋ᵢ)\n\nThe output is the same as fitting the GP on x₋ᵢ,y₋ᵢ,  and calling predict_f on xᵢ to obtain the LOO predictive mean and variance, repeated for each observation i. With GPs, this can thankfully be done analytically with a bit of linear algebra, which is what this function implements.\n\nSee also: predict_CVfold, logp_LOO\n\n\n\n\n\n","category":"method"},{"location":"classification_example/#Binary-classification-1","page":"Binary classification","title":"Binary classification","text":"","category":"section"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"The package is designed to handle models of the following general form","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"beginaligned\nmathbfy    mathbff theta sim  prod_i=1^n p(y_i    f_itheta) \nmathbff     theta sim mathcalGPleft(m_theta(mathbfx) k_theta(mathbfx mathbfx)right)\ntheta  sim p(theta)\nendaligned","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"where mathbfy=(y_1y_2ldotsy_n) in mathcalY and mathbfx in mathcalX are the observations and covariates, respectively, and f_i=f(mathbfx_i) is the latent function which we model with a Gaussian process prior. We assume that the responses mathbfy are independent and identically distributed and as a result the likelihood p(mathbfy    mathbff theta), can be factorized over the observations.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"In the case where the observations are Gaussian distributed, the marginal likelihood and predictive distribution can be derived analytically. See the Regression documentation for an illustration.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"In this example we show how the GP Monte Carlo function can be used for supervised learning classification. We use the Crab dataset from the R package MASS. In this dataset we are interested in predicting whether a crab is of colour form blue or orange. Our aim is to perform a Bayesian analysis and calculate the posterior distribution of the latent GP function mathbff and model parameters theta from the training data mathbfX mathbfy.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"using GaussianProcesses, RDatasets\nimport Distributions:Normal\nusing Random\n\nRandom.seed!(113355)\n\ncrabs = dataset(\"MASS\",\"crabs\");              # load the data\ncrabs = crabs[shuffle(1:size(crabs)[1]), :];  # shuffle the data\n\ntrain = crabs[1:div(end,2),:];\n\ny = Array{Bool}(undef,size(train)[1]);       # response\ny[train[:,:Sp].==\"B\"].=0;                      # convert characters to booleans\ny[train[:,:Sp].==\"O\"].=1;\n\nX = convert(Matrix,train[:,4:end]);          # predictors","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"We assume a zero mean GP with a Matern 3/2 kernel. We use the automatic relevance determination (ARD) kernel to allow each dimension of the predictor variables to have a different length scale. As this is binary classifcation, we use the Bernoulli likelihood,","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"y_i sim mboxBernoulli(Phi(f_i))","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"where Phi mathbbR rightarrow 01 is the cumulative distribution function of a standard Gaussian and acts as a squash function that maps the GP function to the interval [0,1], giving the probability that y_i=1.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"Note that BernLik requires the observations to be of type Bool and unlike some likelihood functions (e.g. student-t) does not contain any parameters to be set at initialisation.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"#Select mean, kernel and likelihood function\nmZero = MeanZero();                # Zero mean function\nkern = Matern(3/2,zeros(5),0.0);   # Matern 3/2 ARD kernel (note that hyperparameters are on the log scale)\nlik = BernLik();                   # Bernoulli likelihood for binary data {0,1}","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"We fit the GP using the general GP function. This function is a shorthand for the GPA function which is used to generate approximations of the latent function when the likelihood is non-Gaussian.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"gp = GP(X',y,mZero,kern,lik)      # Fit the Gaussian process model","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"GP Approximate object:\n  Dim = 5\n  Number of observations = 100\n  Mean function:\n    Type: MeanZero, Params: Float64[]\n  Kernel:\n    Type: Mat32Ard{Float64}, Params: [-0.0, -0.0, -0.0, -0.0, -0.0, 0.0]\n  Likelihood:\n    Type: BernLik, Params: Any[]\n  Input observations =\n[16.2 11.2 … 11.6 18.5; 13.3 10.0 … 9.1 14.6; … ; 41.7 26.9 … 28.4 42.0; 15.4 9.4 … 10.4 16.6]\n  Output observations = Bool[false, false, false, false, true, true, false, true, true, true  …  false, true, false, false, false, true, false, false, false, true]\n  Log-posterior = -161.209","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"We assign Normal priors from the Distributions package to each of the Matern kernel parameters. If the mean and likelihood function also contained parameters, then we could set these priors in the same using gp.m and gp.lik in place of gp.k, respectively.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"set_priors!(gp.kernel,[Normal(0.0,2.0) for i in 1:6])","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"6-element Array{Normal{Float64},1}:\n Normal{Float64}(μ=0.0, σ=2.0)\n Normal{Float64}(μ=0.0, σ=2.0)\n Normal{Float64}(μ=0.0, σ=2.0)\n Normal{Float64}(μ=0.0, σ=2.0)\n Normal{Float64}(μ=0.0, σ=2.0)\n Normal{Float64}(μ=0.0, σ=2.0)","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"Samples of the latent function ftheta    Xy are drawn using MCMC sampling. By default, the mcmc function uses the Hamiltonian Monte Carlo algorithm. Unless defined, the default settings for the MCMC sampler are: 1,000 iterations with no burn-in phase or thinning of the Markov chain.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"samples = mcmc(gp; nIter=10000, burn=1000, thin=10);","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"Number of iterations = 10000, Thinning = 10, Burn-in = 1000\nStep size = 0.100000, Average number of leapfrog steps = 10.015100\nNumber of function calls: 100152\nAcceptance rate: 0.087600","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"We test the predictive accuracy of the fitted model against a hold-out dataset","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"test = crabs[div(end,2)+1:end,:];          # select test data\n\nyTest = Array{Bool}(undef,size(test)[1]);   # test response data\nyTest[test[:,:Sp].==\"B\"].=0;                  # convert characters to booleans\nyTest[test[:,:Sp].==\"O\"].=1;\n\nxTest = convert(Matrix,test[:,4:end]);","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"Using the posterior samples f^(i)theta^(i)_i=1^N from p(ftheta    Xy) we can make predictions haty using the predict_y function to sample predictions conditional on the MCMC samples.","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"ymean = Array{Float64}(undef,size(samples,2),size(xTest,1));\n\nfor i in 1:size(samples,2)\n    set_params!(gp,samples[:,i])\n    update_target!(gp)\n    ymean[i,:] = predict_y(gp,xTest')[1]\nend","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"For each of the posterior samples we plot the predicted observation haty (given as lines) and overlay the true observations from the held-out data (circles).","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"using Plots\ngr()\n\nplot(ymean',leg=false,html_output_format=:png)\nscatter!(yTest)","category":"page"},{"location":"classification_example/#","page":"Binary classification","title":"Binary classification","text":"(Image: png)","category":"page"},{"location":"#GaussianProcesses-1","page":"Introduction","title":"GaussianProcesses","text":"","category":"section"},{"location":"#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Gaussian processes are a family of stochastic processes which provide a flexible nonparametric tool for modelling data. A Gaussian Process places a prior over functions, and can be described as an infinite dimensional generalisation of a multivariate Normal distribution. Moreover, the joint distribution of any finite collection of points is a multivariate Normal. This process can be fully characterised by its mean and covariance functions, where the mean of any point in the process is described by the mean function and the covariance between any two observations is specified by the kernel. Given a set of observed real-valued points over a space, the Gaussian Process is used to make inference on the values at the remaining points in the space.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"For an extensive review of Gaussian Processes there is an excellent book Gaussian Processes for Machine Learning by Rasmussen and Williams, (2006)","category":"page"},{"location":"#Installation-1","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"GaussianProcesses.jl requires Julia version 1.0 or above. To install GaussianProcesses.jl run the following command inside the Julia package REPL","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"pkg> add GaussianProcesses","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"or in the standard REPL","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> using Pkg\njulia> Pkg.add(\"GaussianProcesses\")","category":"page"},{"location":"#Supported-features-1","page":"Introduction","title":"Supported features","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Inference methods\nExact methods based on linear algebra for Gaussian processes with a normal likelihood;\nHamiltonian Monte Carlo for Gaussian processes with any other likelihood;\nElliptical slice sampler for Gaussian processes with a Gaussian likelihood;\nVariational inference for Gaussian processes with any likelihood;\nSparse approximations to accelerate inference through pseudo-inputs;\nHyperparameters\nOptimization of the marginal likelihood for exact GPs;\nPosterior samples of the hyperparameters for likelihoods other than normal;\nMethods to obtain the cross-validation score and its derivative are also available;\nKernels\nBasic kernels such as squared exponential (aka radial basis function), Matérn, etc.;\nSum and product kernels;\nMasked kernels, to apply a kernel to a subset of input dimensions;\nFixed kernels, to prevent optimization of certain kernel hyperparameters;\nAutodifferentation of user-implemented kernels (work in progress);\nMean functions\nBasic mean functions, such as constant, linear, periodic, etc.;\nThe parameters of the mean functions can be fitted by maximum likelihood for exact GPs;\nEasy access to Gaussian process methods\nUnderlying methods like the covariances and their derivatives,","category":"page"},{"location":"#Features-currently-not-implemented-1","page":"Introduction","title":"Features currently not implemented","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"There are many features that we would love to add to this package. If you need one of these features and would be interested in contributing to the package, please get in touch or submit a pull request through GitHub.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Tuning of Hamiltonian Monte Carlo for efficient posterior draws;\nMultivariate Gaussian processes, when the output is a vector, which encompasses multi-task GPs, cokriging, multiclass classification.\nApproximation methods for large GPs:\nexpectation propagation;\nLaplace approximations;","category":"page"}]
}
