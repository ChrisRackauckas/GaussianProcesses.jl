using GaussianProcesses, RDatasets, LinearAlgebra, Statistics, PDMats, Optim, ForwardDiff, Plots
import Distributions:Normal, Poisson
import GaussianProcesses: predict_obs, get_params_kwargs, get_params, predict_f, update_ll_and_dll!, optimize!
using Random
using Optim


mutable struct Approx
    qŒº
    qŒ£
end


abstract type AbstractGradientPrecompute end
abstract type CovarianceStrategy end
struct FullCovariance <: CovarianceStrategy end

struct FullCovMCMCPrecompute <: AbstractGradientPrecompute
    L_bar::Matrix{Float64}
    dl_df::Vector{Float64}
    f::Vector{Float64}
end

function FullCovMCMCPrecompute(nobs::Int)
    buffer1 = Matrix{Float64}(undef, nobs, nobs)
    buffer2 = Vector{Float64}(undef, nobs)
    buffer3 = Vector{Float64}(undef, nobs)
    return FullCovMCMCPrecompute(buffer1, buffer2, buffer3)    
end

function init_precompute(covstrat::FullCovariance, X, y, k)
    nobs = size(X, 2)
    FullCovariancePrecompute(nobs)
end

init_precompute(gp::GPMC) = FullCovMCMCPrecompute(gp.nobs)

function update_Q!(Q::Approx, params::Array)
    Q.qŒº = params[1]
    Q.qŒ£ = params[2]
end


# Possibly unnecessary function for optim
function vector_hessian(f, x)
       n = length(x)
       out = ForwardDiff.jacobian(x -> ForwardDiff.jacobian(f, x), x)
       return reshape(out, n, n, n)
   end


# Compute the Hadamard product
function hadamard(A::Matrix, B::Matrix)
    @assert size(A) == size(B)
    H = Array{Float64}(undef, size(A)) 
    n, m = size(A)
    for j in 1:n
        for i in 1:m
            H[i,j] = A[i, j] * B[i,j]
        end
    end
    return H
end


"""
Compute Œ£ using the Sherman-Woodbury-Morrison identity, such that Œ£=[K^{-1}+Œõ^2]^{-1} = Œõ^{-2} - Œõ^{-1}A^{-1}Œõ^{-1} where A=ŒõKŒõ + I, such that K is the GP's covariance matrix and Œõ=diag(Œª) where Œª is our variational approximation's variance parameters.
"""
# TODO: Remove Q when qŒº and qŒ£ are incorporated into GPMC
function computeŒ£(gp::GPBase, Q::Approx)
    Œõ = diag(Q.qŒ£) .* Matrix(I, size(Q.qŒ£, 1), size(Q.qŒ£, 1))
    A = (Œõ .* gp.cK.mat .* Œõ) .+ (Matrix(I, gp.nobs, gp.nobs)*1.0)
    Œ£ = Œõ.^(-2) .- (Œõ^(-1) .* A^(-1) .* Œõ^(-1))
    return Œ£
end


"""
Compute the gradient of the ELBO F w.r.t. the variational parameters Œº and Œ£, as per Equations (11) and (12) in Opper and Archambeau.
"""
function elbo_grad_q(gp::GPBase, Q::Approx)
    ŒΩbar = -gp.dll[1:gp.nobs]
    gŒΩ = gp.cK.mat*(Q.qŒº - ŒΩbar) # TODO: Should this be a product of the application of the covariance function to ŒΩ-ŒΩbar?
    Œ£ = computeŒ£(gp, Q)
    Œª = Q.qŒ£
    # Œªbar = 
    gŒª = diag(0.5*(hadamard(Œ£, Œ£))) # Must multiply by Œª-Œªbar
    return gŒΩ, gŒª
end


function elbo_grad_Œ∏(gp::GPBase)
   # TODO: Can ŒΩ just equal ŒΩbar, as per Section 4?
   ŒΩbar = gp.dll[1:gp.nobs]
   
   # Computing EQ16 of Opper
   ‚àáŒ∏ = -0.5*(dot(ŒΩbar, ŒΩbar) .- inv(gp.cK.mat))
   print(‚àáŒ∏)
end


"""
Update the parameters of the variational approximation through gradient ascent
"""
function updateQ!(Q::Approx, ‚àáŒº, ‚àáŒ£; Œ±::Float64=0.01)
    Q.qŒº += Œ±*-‚àáŒº
    Q.qŒ£ += Œ±*-‚àáŒ£ .* (Matrix(I, length(‚àáŒ£), length(‚àáŒ£)) *1.0)
end


"""
Set the GP's posterior distribution to be the multivariate Gaussian approximation.
"""
function approximate!(gp::GPBase, Q::Approx)
end


"""
Carry out variational inference, as per Opper and Archambeau (2009) to compute the GP's posterior, given a non-Gaussian likelihood.
"""
function vi(gp::GPBase; verbose::Bool=false, nits::Int=100, plot_elbo::Bool=false)
    # Initialise log-target and log-target's derivative
    mcmc(gp; nIter=1)
    optimize!(gp)
    Q = Approx(gp.v, Matrix(I, gp.nobs, gp.nobs)*1.0)
    # Initialise the varaitaional parameters
#    Q = Approx(zeros(gp.nobs), Matrix(I, gp.nobs, gp.nobs)*1.0)
    # Compute the initial ELBO objective between the intiialised Q and the GP
    Œª = [zeros(gp.nobs), Matrix(I, gp.nobs, gp.nobs)*1.0]
    

    function elbo(params)
        # Compute the prior KL e.g. KL(Q||P) s.t. P‚àºN(0, I)
        kl = 0.5(dot(Q.qŒº, Q.qŒº) - logdet(Q.qŒ£) + sum(diag(Q.qŒ£).^2))
        @assert kl >= 0 "KL-divergence should be positive.\n"
        println("KL: ", kl)
        Œº = mean(gp.mean, gp.x)
        Œ£= cov(gp.kernel, gp.x, gp.data)    #kernel function
        gp.cK = PDMat(Œ£ + 1e-6*I)
        Fmean = unwhiten(gp.cK, Q.qŒº) + Œº      # K‚Åª¬πq_Œº

        # Assuming a mean-field approximation
        Fvar = diag(unwhiten(gp.cK, Q.qŒ£))              # K‚Åª¬πq_Œ£
        _, varExp = predict_obs(gp.lik, Fmean, Fvar)      # ‚à´log p(y|f)q(f), where q(f) is a Gaussian approx.
        # ELBO = Œ£_n ùîº_{q(f_n)} ln p(y_n|f_n) + KL(q(f)||p(f))
        elbo_val = sum(varExp)-kl
        println("ELBO: ", elbo_val)
        # @assert elbo_val <= 0 "ELBO Should be less than 0.\n"
        return sum(varExp) - kl
    end
    init_elbo = elbo(Œª) # TODO: Change this Œª
    if verbose
        println("Initial ELBO: ", init_elbo)
    end
    
    global elbo_approx = Array{Float64}(undef, nits+1)
    elbo_approx[1] = init_elbo

    # Iteratively update variational parameters
    for i in 1:nits
        buff = init_precompute(gp)
        update_ll_and_dll!(gp, buff)
        Œª = [Q.qŒº, Q.qŒ£]

        # Compute the gradients of the variational objective function
        gradŒº, gradŒ£ = elbo_grad_q(gp, Q)

        # Update the variational parameters
        updateQ!(Q, gradŒº, gradŒ£)

        # Recalculate the ELBO
        Œª = [Q.qŒº, Q.qŒ£]
        current_elbo = elbo(Œª)
        elbo_approx[i+1] = current_elbo

        if verbose
            println("ELBO at Iteration ", i, ": ", current_elbo)
        end
    end

    if plot_elbo
        println(elbo_approx)
        # plot(0:nits, elbo_approx)
    end
end


Random.seed!(123)

n = 20
X = collect(range(-3,stop=3,length=n));
f = 2*cos.(2*X);
Y = [rand(Poisson(exp.(f[i]))) for i in 1:n];

#GP set-up
k = Matern(3/2,0.0,0.0)   # Matern 3/2 kernel
l = PoisLik()             # Poisson likelihood

gp = GP(X, vec(Y), MeanZero(), k, l)
set_priors!(gp.kernel,[Normal(-2.0,4.0),Normal(-2.0,4.0)])

# mcmc(gp)
vi(gp;nits=10, verbose=true, plot_elbo=true)
