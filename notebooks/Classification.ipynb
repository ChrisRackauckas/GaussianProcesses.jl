{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with the crabs data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is designed to handle models of the following general form\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{y} \\ |\\ \\mathbf{f}, \\theta &\\sim  \\prod_{i=1}^n p(y_i \\ | \\ f_i,\\theta), \\\\\n",
    "    \\mathbf{f} \\ | \\ \\theta &\\sim \\mathcal{GP}\\left(m_{\\theta}(\\mathbf{x}), k_{\\theta}(\\mathbf{x}, \\mathbf{x}')\\right),\\\\\n",
    "      \\theta &\\sim p(\\theta), \n",
    "\\end{aligned}$$\n",
    "where $\\mathbf{y}=(y_1,y_2,\\ldots,y_n) \\in \\mathcal{Y}$ and $\\mathbf{x} \\in \\mathcal{X}$ are the observations and covariates, respectively, and $f_i:=f(\\mathbf{x}_i)$ is the latent function which we model with a Gaussian process prior. We assume that the responses $\\mathbf{y}$ are independent and identically distributed and as a result the likelihood $p(\\mathbf{y} \\ | \\ \\mathbf{f}, \\theta)$, can be factorized over the observations. \n",
    "\n",
    "In the case where the observations are Gaussian distributed, the marginal likelihood and predictive distribution can be derived analytically. See the  [Regression notebook](https://github.com/STOR-i/GaussianProcesses.jl/blob/master/notebooks/Regression.ipynb) for an illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we show how the GP **Monte Carlo** function can be used for **supervised learning classification**. We use the Crab dataset from the R package MASS. In this dataset we are interested in predicting whether a crab is of colour form blue or orange. Our aim is to perform a Bayesian analysis and calculate the posterior distribution of the latent GP function $\\mathbf{f}$ and model parameters $\\theta$ from the training data $\\{\\mathbf{X}, \\mathbf{y}\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using GaussianProcesses, RDatasets\n",
    "import Distributions:Normal\n",
    "\n",
    "srand(113355)\n",
    "\n",
    "crabs = dataset(\"MASS\",\"crabs\");              # load the data \n",
    "crabs = crabs[shuffle(1:size(crabs)[1]), :];  # shuffle the data\n",
    "\n",
    "train = crabs[1:div(end,2),:];\n",
    "\n",
    "y = Array{Bool}(size(train)[1]);           # response\n",
    "y[train[:Sp].==\"B\"]=0;                      # convert characters to booleans\n",
    "y[train[:Sp].==\"O\"]=1;\n",
    "\n",
    "X = convert(Array,train[:,4:end]);          # predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a zero mean GP with a Matern 3/2 kernel. We use the automatic relevance determination (ARD) kernel to allow each dimension of the predictor variables to have a different length scale. As this is binary classifcation, we use the Bernoulli likelihood, \n",
    "\n",
    "$$\n",
    "y_i \\sim Bernoulli(\\Phi(f_i))\n",
    "$$\n",
    "where $\\Phi: \\mathbb{R} \\rightarrow [0,1]$ is the cumulative distribution function of a standard Gaussian and acts as a squash function that maps the GP function to the interval [0,1], giving the probability that $y_i=1$. \n",
    "\n",
    "**Note** that `BernLik` requires the observations to be of type `Bool` and unlike some likelihood functions (e.g. student-t) does not contain any parameters to be set at initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select mean, kernel and likelihood function\n",
    "mZero = MeanZero();                # Zero mean function\n",
    "kern = Matern(3/2,zeros(5),0.0);   # Matern 3/2 ARD kernel (note that hyperparameters are on the log scale)\n",
    "lik = BernLik();                   # Bernoulli likelihood for binary data {0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the GP using the general `GP` function. This function is a shorthand for the `GPMC` function which is used to generate **Monte Carlo approximations** of the latent function when the **likelihood is non-Gaussian**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GP Monte Carlo object:\n",
       "  Dim = 5\n",
       "  Number of observations = 100\n",
       "  Mean function:\n",
       "    Type: GaussianProcesses.MeanZero, Params: Float64[]\n",
       "  Kernel:\n",
       "    Type: GaussianProcesses.Mat32Ard, Params: [-0.0, -0.0, -0.0, -0.0, -0.0, 0.0]\n",
       "  Likelihood:\n",
       "    Type: GaussianProcesses.BernLik, Params: Any[]\n",
       "  Input observations = \n",
       "[16.2 11.2 … 11.6 18.5; 13.3 10.0 … 9.1 14.6; … ; 41.7 26.9 … 28.4 42.0; 15.4 9.4 … 10.4 16.6]\n",
       "  Output observations = Bool[false, false, false, false, true, true, false, true, true, true  …  false, true, false, false, false, true, false, false, false, true]\n",
       "  Log-posterior = -161.209"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = GP(X',y,mZero,kern,lik)      # Fit the Gaussian process model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign `Normal` priors from the `Distributions` package to each of the Matern kernel parameters. If the mean and likelihood function also contained parameters, then we could set these priors in the same using `gp.m` and `gp.lik` in place of `gp.k`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Distributions.Normal{Float64},1}:\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)\n",
       " Distributions.Normal{Float64}(μ=0.0, σ=2.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_priors!(gp.k,[Normal(0.0,2.0) for i in 1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the posterior distribution of the latent function and parameters $f, \\theta | X,y$, are drawn using MCMC sampling. The `mcmc` function uses a Hamiltonain Monte Carlo sampler. By default, the function runs for `nIter=1000` iterations, uses a step-size of $\\epsilon=0.01$ with a random number of leap-frog steps $L$ between 5 and 15. Setting `Lmin` and `Lmax` gives the MALA algorithm. Finally, by default, the sampler also prints the current iteration number every 100 iterations, this can be turned off with `verbose=false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "DomainError:",
     "output_type": "error",
     "traceback": [
      "DomainError:",
      "",
      "Stacktrace:",
      " [1] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Distances/src/common.jl:96\u001b[22m\u001b[22m [inlined]",
      " [2] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./simdloop.jl:73\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1msqrt!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Distances/src/common.jl:95\u001b[22m\u001b[22m",
      " [4] \u001b[1mdistance\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::GaussianProcesses.Mat32Ard, ::Array{Float64,2}, ::GaussianProcesses.StationaryARDData\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/kernels/stationary.jl:137\u001b[22m\u001b[22m",
      " [5] \u001b[1mcov!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::GaussianProcesses.Mat32Ard, ::Array{Float64,2}, ::GaussianProcesses.StationaryARDData\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/kernels/stationary.jl:40\u001b[22m\u001b[22m",
      " [6] \u001b[1mupdate_ll!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::GaussianProcesses.GPMC{Bool}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/GPMC.jl:84\u001b[22m\u001b[22m",
      " [7] \u001b[1mupdate_target!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::GaussianProcesses.GPMC{Bool}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/GPMC.jl:159\u001b[22m\u001b[22m",
      " [8] \u001b[1m(::GaussianProcesses.#logpost#108{GaussianProcesses.GPMC{Bool}})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/mcmc2.jl:20\u001b[22m\u001b[22m",
      " [9] \u001b[1m(::Klara.##328#366{Tuple{Void,Void,Void,Void,GaussianProcesses.#logpost#108{GaussianProcesses.GPMC{Bool}},Void,Void,GaussianProcesses.#dlogpost#109{GaussianProcesses.GPMC{Bool}},Void,Void,Void,Void,Void,Void,Void,Void,Void},Array{Symbol,1}})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.BasicContMuvParameterState{Float64}, ::Array{Klara.VariableState,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/variables/parameters/BasicContMuvParameter.jl:529\u001b[22m\u001b[22m",
      " [10] \u001b[1m(::Klara.##274#291{BasicContMuvParameter,Tuple{Void,Void,Void,Void,Klara.##328#366{Tuple{Void,Void,Void,Void,GaussianProcesses.#logpost#108{GaussianProcesses.GPMC{Bool}},Void,Void,GaussianProcesses.#dlogpost#109{GaussianProcesses.GPMC{Bool}},Void,Void,Void,Void,Void,Void,Void,Void,Void},Array{Symbol,1}},Void,Void,Klara.##328#366{Tuple{Void,Void,Void,Void,GaussianProcesses.#logpost#108{GaussianProcesses.GPMC{Bool}},Void,Void,GaussianProcesses.#dlogpost#109{GaussianProcesses.GPMC{Bool}},Void,Void,Void,Void,Void,Void,Void,Void,Void},Array{Symbol,1}},Void,Void,Void,Void,Void,Void,Void,Void,Void}})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.BasicContMuvParameterState{Float64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/variables/parameters/BasicContMuvParameter.jl:182\u001b[22m\u001b[22m",
      " [11] \u001b[1m(::Klara.##282#299{BasicContMuvParameter})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.BasicContMuvParameterState{Float64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/variables/parameters/BasicContMuvParameter.jl:272\u001b[22m\u001b[22m",
      " [12] \u001b[1minitialize!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.BasicContMuvParameterState{Float64}, ::BasicContMuvParameter, ::Klara.HMC, ::Dict{Symbol,Any}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/samplers/HMC.jl:112\u001b[22m\u001b[22m",
      " [13] \u001b[1mKlara.BasicMCJob\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.GenericModel, ::Int64, ::Klara.HMC, ::Klara.VanillaMCTuner, ::Klara.BasicMCRange{Int64}, ::Array{Klara.VariableState,1}, ::Dict{Symbol,Any}, ::Bool, ::Bool, ::Bool\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/jobs/BasicMCJob.jl:73\u001b[22m\u001b[22m",
      " [14] \u001b[1m(::Core.#kw#Type)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Type{Klara.BasicMCJob}, ::Klara.GenericModel, ::Klara.HMC, ::Klara.BasicMCRange{Int64}, ::Array{Any,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [15] \u001b[1m#BasicMCJob#448\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Int64, ::Klara.VanillaMCTuner, ::Dict{Symbol,Any}, ::Bool, ::Bool, ::Bool, ::Type{T} where T, ::Klara.GenericModel, ::Klara.HMC, ::Klara.BasicMCRange{Int64}, ::Dict{Symbol,Array{Float64,1}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/jobs/BasicMCJob.jl:169\u001b[22m\u001b[22m",
      " [16] \u001b[1mKlara.BasicMCJob\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.GenericModel, ::Klara.HMC, ::Klara.BasicMCRange{Int64}, ::Dict{Symbol,Array{Float64,1}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/Klara/src/jobs/BasicMCJob.jl:164\u001b[22m\u001b[22m",
      " [17] \u001b[1m#mcmc#107\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Klara.HMC, ::Int64, ::Int64, ::Int64, ::Function, ::GaussianProcesses.GPMC{Bool}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/nobackup/nemeth/.julia/v0.6/GaussianProcesses/src/mcmc2.jl:34\u001b[22m\u001b[22m",
      " [18] \u001b[1m(::GaussianProcesses.#kw##mcmc)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::GaussianProcesses.#mcmc, ::GaussianProcesses.GPMC{Bool}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [19] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "samples = mcmc(gp;sampler=Klara.HMC(),nIter=5000,burnin=1000,thin=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the predictive accuracy of the fitted model against a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = crabs[div(end,2)+1:end,:];          # select test data\n",
    "\n",
    "yTest = Array{Bool}(size(test)[1]);        # test response data\n",
    "yTest[test[:Sp].==\"B\"]=0;                  # convert characters to booleans\n",
    "yTest[test[:Sp].==\"O\"]=1;\n",
    "\n",
    "xTest = convert(Array,test[:,4:end]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior samples $\\{f^{(i)},\\theta^{(i)}\\}_{i=1}^N$ from $p(f,\\theta \\ | \\ X,y)$ we can make predictions $\\hat{y}$ using the `predict_y` function to sample predictions conditional on the MCMC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: thinned_samples not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: thinned_samples not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "ymean = Array{Float64}(size(thinned_samples,2),size(xTest,1));\n",
    "\n",
    "for i in 1:size(thinned_samples,2)\n",
    "    set_params!(gp,thinned_samples[:,i])\n",
    "    update_target!(gp)\n",
    "    ymean[i,:] = predict_y(gp,xTest')[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the posterior samples we plot the predicted observation $\\hat{y}$ (given as lines) and overlay the true observations from the held-out data (circles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: ymean not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: ymean not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "gr()\n",
    "\n",
    "plot(ymean',leg=false,html_output_format=:png)\n",
    "scatter!(yTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
